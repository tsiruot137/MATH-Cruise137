\documentclass{article}

    \usepackage{xcolor}
    \definecolor{pf}{rgb}{0.4,0.6,0.4}
    \usepackage[top=1in,bottom=1in, left=0.8in, right=0.8in]{geometry}
    \usepackage{setspace}
    \setstretch{1.2} 
    \setlength{\parindent}{0em}

    \usepackage{paralist}
    \usepackage{cancel}

    % \usepackage{ctex}
    \usepackage{amssymb}
    \usepackage{amsmath}

    \usepackage{tcolorbox}
    \definecolor{Df}{RGB}{0, 184, 148}
    \definecolor{Th}{RGB}{9, 132, 227}
    \definecolor{Rmk}{RGB}{215, 215, 219}
    \definecolor{P}{RGB}{154, 13, 225}
    \newtcolorbox{Df}[2][]{colbacktitle=Df, colback=white, title={\large\color{white}#2},fonttitle=\bfseries,#1}
    \newtcolorbox{Th}[2][]{colbacktitle=Th, colback=white, title={\large\color{white}#2},fonttitle=\bfseries,#1}
    \newtcolorbox{Rmk}[2][]{colbacktitle=Rmk, colback=white, title={\large\color{black}{Remarks}},fonttitle=\bfseries,#1}

    \title{\LARGE \textbf{Differentiation on $\mathbb{R}^n$}}
    \author{\large Jiawei Hu}

    % new commands for formula typying
    \newcommand{\parfrac}[2]{\frac{\partial #1}{\partial #2}}
    \newcommand{\biparfrac}[2]{\frac{\partial^2 #1}{#2}}
    \newcommand{\dif}{\mathop{}\!\mathrm{d}}
    \newcommand{\Dif}{\mathop{}\!\mathrm{D}}
\begin{document}
\maketitle

This is the 5th chapter of Mathematical Analysis, which is about \textbf{Differentiation on $\mathbb{R}^n$}. By the way, we now pre-claim some commonly-used notations and terms:
\begin{Df}{Notations and Terms}
    \begin{compactenum}
        \item $\mathbb{R}$: the set of the real numbers; $\mathbb{R}_\infty = \mathbb{R}\cup\{-\infty, \infty\}$;
        \item $\mathbb{R}^n$: without extra specification, $n\in\mathbb{N}^\ast$; 
        \item You may see some statements like ``this Df/Th is merely an extension of the previous one'', which means that the current Df/Th can be reduced to the previous one on some conditions.
        \item An agreement for the length of a list: if we write $a_1, \dots, a_n$, then we indicate that $n$ is finite and that $n\geq 1$; if we write $a_0, \dots, a_n$, then we indicate that $n$ is finite and that $n\geq 0$.
        \item Keep coincident in the notions and notations of functions with the chapter 1 of course 0, including the ones of domain, range, restriction, image, pre-image, inverse and composition. Specifically for a function $f: A\rightarrow B$ and some sets $E\subseteq A$ and $F\subseteq B$, the image of $E$ and the pre-image of $F$ under $f$ are just:
        $$f[E] = \{f(x): x\in E\},\quad f^{-1}[F] = \{x\in A: f(x)\in F\}$$
        \item For the existence of a limit, if we have used the symbol $\lim\limits_{x\to x_0} f(x)$ in an expression (such as an equality, an inequality or some expressions involving some other numbers), then without explicitly specification, we imply that the limit exists (``exist'' means finite according to the chapter 1).
        \item The inner product and norm in $\mathbb{R}^n$ are the typical ones: $\langle \pmb{x}, \pmb{y}\rangle = x_1y_1 + \dots + x_ny_n$, $\Vert \pmb{x}\Vert = \sqrt{\langle \pmb{x}, \pmb{x}\rangle}$.
        \item $E^c$: Let $E\subseteq\mathbb{R}^n$. Then $E^c\triangleq \mathbb{R}^n\setminus E$.
        \item A set of sets is called a collection or a family.
        \item A vector in $\mathbb{R}^n$, without explicit specification, is written as a column vector, i.e. an $n\times 1$ matrix.
    \end{compactenum}
\end{Df}

Here is the \textbf{Quick Search} for this chapter:
\begin{Th}{Quick Search}
    \begin{compactdesc}
        \item (5.1.*): Directional Derivatives and partial derivatives.
        \item (5.2.*): Differential.
        \item (5.3.1.*): Theorem of implicit function.
        \item (5.3.2.*): Theorem of inverse function.
        \item (5.4.*): Partial derivatives of higher orders.
    \end{compactdesc}
\end{Th}

Then with everything prepared, here we go.

\begin{Df}{Df5.1.1.-1 (direction)}
    Suppose $\pmb{u}\in\mathbb{R}_n$. Then $\pmb{u}$ is called a direction in $\mathbb{R}_n$ if $\Vert \pmb{u}\Vert = 1$.
\end{Df}

\begin{Th}{Th5.1.1.0.-1 ($\Vert A\Vert$ and its properties)}
    This is a copy-back of Df \{, ID: 5.2.8.-1\} and Th \{, ID: 5.2.8.-1.1\}.
\end{Th}

\begin{Df}{Df5.1.1 (directional derivative)}
    Suppose $f$ is an $n$-real function and $\pmb{x}_0\in (\text{dom}(f))^\circ$. Suppose also $\pmb{u}$ is a direction in $\mathbb{R}_n$. If the limit
    $$\lim_{t\to 0}\frac{f(\pmb{x}_0 + t\pmb{u}) - f(\pmb{x}_0)}{t}$$ 
    exists, then it is called the \textbf{directional derivative} of $f$ at $\pmb{x}_0$ in the direction $\pmb{u}$, denoted by $\parfrac{f}{\pmb{u}}(\pmb{x}_0)$ (or $\parfrac{y}{\pmb{u}}(\pmb{x}_0)$ if we write the dependent variable $f(\pmb{x})$ as $y$), $\parfrac{f}{\pmb{u}}(\pmb{x}_0)$ is said to exist, and $f$ is said to be derivable at $\pmb{x}_0$ in the direction $\pmb{u}$.
\end{Df}

\begin{Rmk}{}
    \begin{compactenum}
        \item \textcolor{Th}{This definition is an extension of the one of the derivative of real functions. In $\mathbb{R}$, the directions are just $1$ or $-1$, and $\parfrac{f}{u}(x_0)$ in the direction $u=1$ is just $f'(x_0)$.}
        \item \textcolor{Df}{Suppose $f$ is an $n$-real function and $\pmb{u}$ is a direction. If $\parfrac{f}{\pmb{u}}(\pmb{x})$ exists for every $\pmb{x}$ in some set $E\subseteq\mathbb{R}_n$, then we say that $f$ is derivable in the direction $\pmb{u}$ on $E$.}
        \item \textcolor{Th}{Suppose $f$ is an $n$-real function derivable at $\pmb{x}_0$ in the direction $\pmb{u}$. Let $\varphi(t) = f(\pmb{x}_0 + t\pmb{u})$, then $\varphi'(0) = \parfrac{f}{\pmb{u}}(\pmb{x}_0)$.}
        \item \textcolor{Th}{Suppose $f$ is an $n$-real function and $\pmb{x}_0\in (\text{dom}(f))^\circ$. Then for any direction $\pmb{u}$ in $\mathbb{R}_n$, $\parfrac{f}{\pmb{u}}(\pmb{x}_0)$ coexists with $\parfrac{f}{(-\pmb{u})}(\pmb{x}_0)$ and $\parfrac{f}{(-\pmb{u})}(x_0) = -\parfrac{f}{\pmb{u}}(x_0)$.}
        \item Obviously the arithmetics holds. \textcolor{Th}{The following equalities mean that if the right side can be computed, then so is the left side, and the left equals right: (let $\pmb{u}$ is a direction)
        \begin{compactenum}
            \item $\parfrac{(f\pm g)}{\pmb{u}}(\pmb{x}_0) = \parfrac{f}{\pmb{u}}(\pmb{x}_0)\pm\parfrac{g}{\pmb{u}}(\pmb{x}_0)$;
            \item $\parfrac{(fg)}{\pmb{u}}(\pmb{x}_0) = \parfrac{f}{\pmb{u}}(\pmb{x}_0) g(\pmb{x}_0) + f(\pmb{x}_0)\parfrac{g}{\pmb{u}}(\pmb{x}_0)$;
            \item $\parfrac{(f/g)}{\pmb{u}}(\pmb{x}_0) = [g(\pmb{x}_0)]^{-2} \bigl[\parfrac{f}{\pmb{u}}(\pmb{x}_0)g(\pmb{x}_0) - f(\pmb{x}_0)\parfrac{g}{\pmb{u}}(\pmb{x}_0)\bigr]$.
        \end{compactenum}
        which shows no difference from the derivative of real functions.}
    \end{compactenum}
\end{Rmk}

\begin{Df}{Df5.1.2 (partial derivative)}
    Suppose $f$ is an $n$-real function and $\pmb{x}_0\in (\text{dom}(f))^\circ$. Then the directional derivatives $\parfrac{f}{\pmb{e}_i}(\pmb{x}_0)$, $i = 1, \dots, n$, are called the \textbf{partial derivatives} of $f$ at $\pmb{x}_0$, denoted by 
    $$\Dif_i f(\pmb{x}_0) = \parfrac{f}{\pmb{e}_i}(\pmb{x}_0).$$ 
    Here $\pmb{e}_i$ is the $i$-th one-hot vector in $\mathbb{R}^n$, and $\Dif_i f(\pmb{x}_0)$ is called the $i$-th partial derivative of $f$ at $\pmb{x}_0$. If we write $y = f(\pmb{x}) = f(x_1, \cdots, x_n)$, then $\Dif_i f(\pmb{x}_0)$ is also denoted by $\parfrac{f}{x_i}(\pmb{x}_0)$ or $\parfrac{y}{x_i}(\pmb{x}_0)$.
\end{Df}

\begin{Df}{Df5.1.3 (directional derivative functions)}
    Suppose $f$ is an $n$-real function derivable in some direction $\pmb{u}$ on $D\subseteq\mathbb{R}_n$. Then the function $\pmb{x}\mapsto \parfrac{f}{\pmb{u}}(\pmb{x})$ defined on $D$ is called the directional derivative function of $f$ in the direction $\pmb{u}$ on $D$.
\end{Df}

\begin{Rmk}{}
    \textcolor{Df}{If $D$ in this definition equals $\text{dom}(f)$, we say that $f$ is a derivable function in the direction $\pmb{u}$.}
\end{Rmk}

\begin{Df}{Df5.1.4 (gradient)}
    Suppose $f$ is an $n$-real function and $\pmb{x}_0\in (\text{dom}(f))^\circ$. Then the $1\times n$ matrix 
    $$(\Dif_1 f(\pmb{x}_0), \cdots, \Dif_n f(\pmb{x}_0))$$
    is called the \textbf{gradient} or the \textbf{Jacobian matrix} of $f$ at $\pmb{x}_0$, denoted by $\text{grad}\,f(\pmb{x}_0)$ or $\pmb{J}f(\pmb{x}_0)$.
\end{Df}

\begin{Rmk}{}
    We certainly have the arithmetics for the gradient \textcolor{Th}{(still for each equality, the left side can be computed if the right side can, and the left equals right): 
    \begin{compactenum}
        \item $\pmb{J}(f\pm g)(\pmb{x}_0) = \pmb{J}f(\pmb{x}_0)\pm\pmb{J}g(\pmb{x}_0)$;
        \item $\pmb{J}(fg)(\pmb{x}_0) = \pmb{J}f(\pmb{x}_0)g(\pmb{x}_0) + f(\pmb{x}_0)\pmb{J}g(\pmb{x}_0)$;
        \item $\pmb{J}(f/g)(\pmb{x}_0) = [g(\pmb{x}_0)]^{-2} \bigl[\pmb{J}f(\pmb{x}_0)g(\pmb{x}_0) - f(\pmb{x}_0)\pmb{J}g(\pmb{x}_0)\bigr]$.
    \end{compactenum}
    }
\end{Rmk}

\begin{Df}{Df5.2.1 (differential)}
    Suppose $f$ is an $n$-real function and $\pmb{x}_0\in (\text{dom}(f))^\circ$. If there exists some $\pmb{\lambda}\in\mathbb{R}^{1,n}$ s.t. the differential equality
    $$ f(\pmb{x}_0 + \pmb{h}) - f(\pmb{x}_0) = \pmb{\lambda}\pmb{h} + R(\pmb{h}) $$
    (where 
    $$\lim_{\pmb{h}\to \pmb{0}}\frac{R(\pmb{h})}{\Vert \pmb{h}\Vert} = 0 $$
    ) holds for $\pmb{h}$ in some $B_\delta(\pmb{0})$. Then the linear function $\pmb{\lambda h}$ of $\pmb{h}$ is called the \textbf{differential} of $f$ at $\pmb{x}_0$, denoted by $\dif f(\pmb{x}_0)$, and $f$ is said to be differentiable at $\pmb{x}_0$.
\end{Df}

\begin{Rmk}{}
    It is easy to verify \textcolor{Th}{the uniqueness of differential:} if $f(\pmb{x}_0 + \pmb{h}) - f(\pmb{x}_0) = \pmb{\lambda}_1\pmb{h} + R_1(\pmb{h}) = \pmb{\lambda}_2\pmb{h} + R_2(\pmb{h})$, then $(\pmb{\lambda}_1 - \pmb{\lambda}_2)\pmb{h} = R_2(\pmb{h}) - R_1(\pmb{h}) = o(\Vert \pmb{h}\Vert)$, (\textcolor{Df}{where $o(\Vert \pmb{h}\Vert)$ merely extends the notation of $o(\cdot)$ in $\mathbb{R}$, i.e. infinitely small of a higher order.}) and this is impossible unless $\pmb{\lambda}_1 = \pmb{\lambda}_2$. \\
    \textcolor{Df}{$f$ is said to be differentiable on $D$ if $f$ is differentiable at every point in $D$. And we will still keep this agreement for the $n$-real $m$-functions (talked about later).}
\end{Rmk}

\begin{Th}{Th5.2.1.1 (differential is gradient)}
    Suppose $f$ is an $n$-real function. If $f$ is differentiable at $\pmb{x}_0$, then the gradient of $f$ at $\pmb{x}_0$ exists and
    $$\dif f(\pmb{x}_0) = \pmb{J}f(\pmb{x}_0)\,\pmb{h}.$$
    \tcblower
    \textit{Pf}: Obvious. Suppose $\dif f(\pmb{x}_0) = \pmb{\lambda h}$ where $\pmb{\lambda} = (\lambda_1, \cdots, \lambda_n)^\mathrm{T}$. Let $\pmb{h}$ be, say, $(h_1, 0, \cdots, 0)^\mathrm{T}$, then the differential equality just implies $\lambda_1 = \Dif_1 f(\pmb{x}_0)$.
\end{Th}

\begin{Rmk}{}
    We certainly have the arithmetics for the differential \textcolor{Th}{(still for each equality, the left side can be computed if the right side can, and the left equals right):
    \begin{compactenum}
        \item $\dif(f\pm g)(\pmb{x}_0) = \dif f(\pmb{x}_0)\pm\dif g(\pmb{x}_0)$;
        \item $\dif(fg)(\pmb{x}_0) = \dif f(\pmb{x}_0)g(\pmb{x}_0) + f(\pmb{x}_0)\dif g(\pmb{x}_0)$;
        \item $\dif(f/g)(\pmb{x}_0) = [g(\pmb{x}_0)]^{-2} \bigl[\dif f(\pmb{x}_0)g(\pmb{x}_0) - f(\pmb{x}_0)\dif g(\pmb{x}_0)\bigr]$.
    \end{compactenum}
    }
\end{Rmk}

\begin{Th}{Th5.2.2 (differentiable $\Rightarrow$ continuous)}
    Suppose $f$ is an $n$-real function. If $f$ is differentiable at $\pmb{x}_0$, then $f$ is continuous at $\pmb{x}_0$.
    \tcblower
    \textit{Pf}: Obvious. Just let $\pmb{h}\rightarrow \pmb{0}$ in the differential equality.
\end{Th}

\begin{Th}{Th5.2.3 (linear decomposition for $R(\pmb{h})$)}
    Suppose $f$ is an $n$-real function and $\pmb{x}_0\in (\text{dom}(f))^\circ$. Then $f$ is differentiable at $\pmb{x}_0$ iff the equality
    $$ f(\pmb{x}_0 + \pmb{h}) - f(\pmb{x}_0) = \pmb{J}f(\pmb{x}_0)\,\pmb{h} + \pmb{\beta}(\pmb{h})\,\pmb{h}, $$
    (where $\pmb{\beta}(\pmb{h}) = (\beta_1(\pmb{h}), \cdots, \beta_n(\pmb{h}))$ converges to $\pmb{0}$ as $\pmb{h}\rightarrow \pmb{0}$) holds (for $\pmb{h}$ in some $B_\delta(\pmb{0})$).
    \tcblower
    \textit{Pf}: ``if'' is obvious. For ``only if'', let the remainder $R(\pmb{h}) = f(\pmb{x}_0 + \pmb{h}) - f(\pmb{x}_0) - \pmb{J}f(\pmb{x}_0)\,\pmb{h}$, then $R(\pmb{h}) = o(\Vert \pmb{h}\Vert)$, and we can write $R(\pmb{h}) = \pmb{\beta}(\pmb{h})\,\pmb{h}$ by
    $$\beta_i(\pmb{h}) = \frac{R(\pmb{h})h_i}{\Vert \pmb{h}\Vert^2}, \quad (i=1,\cdots,n).$$
\end{Th}

\begin{Df}{Df5.2.4.-1 (neighborhood)}
    Suppose $\pmb{x}_0\in\mathbb{R}^n$ and $E\subseteq\mathbb{R}^n$. Then $E$ is called a \textbf{neighborhood} of $\pmb{x}_0$ if $E$ is open and $\pmb{x}_0\in E$.
\end{Df}

\begin{Th}{Th5.2.4 (gradient exists near $\pmb{x}_0$ and continuous at $\pmb{x}_0$ $\Rightarrow$ differentiable)}
    Suppose $f$ is an $n$-real function. If for all $i=1,\cdots,n$, 
    \begin{compactenum}
        \item $\Dif_i f(\pmb{x})$ exists for $\pmb{x}$ in some neighborhood of $\pmb{x}_0$;
        \item $\Dif_i f(\cdot)$ is continuous at $\pmb{x}_0$,
    \end{compactenum}
    then $f$ is differentiable at $\pmb{x}_0$.
    \tcblower
    \textit{Pf}: Induction on $n$. The proposition holds for $n=1$. Suppose it holds for all $k=1,\cdots, n$, then for $n+1$, we decompose the increment of $f(\pmb{x}_0)$ as:
    $$ f(\pmb{x}_0 + \pmb{h}) - f(\pmb{x}_0) = K_1 + K_2, $$
    where
    $$
    \begin{aligned}
    K_1 &= f(x_1+h_1, \cdots, x_n+h_n, x_{n+1} + h_{n+1}) - f(x_1+h_1, \cdots, x_n+h_n, x_{n+1}),\\
    K_2 &= f(x_1+h_1, \cdots, x_n+h_n, x_{n+1}) - f(x_1, \cdots, x_n, x_{n+1}).
    \end{aligned}
    $$
    According to the induction hypothesis, $f$ is differentiable at $\pmb{x}_0$ after $x_{n+1}$ is fixed, and thus by Th \{, ID: 5.2.3\}, 
    $$ K_2 = \sum_{i=1}^{n} \parfrac{f}{x_i}(\pmb{x}_0)h_i + \sum_{i=1}^{n} \beta_i (\pmb{h}) h_i, $$
    where $\beta_i(\pmb{h})\rightarrow 0$ as $\pmb{h} = (h_1, \cdots, h_n, h_{n+1})\rightarrow \pmb{0}$.
    For $K_1$, apply the Lagrange intermediate value theorem:
    $$ K_1 = \parfrac{f}{x_{n+1}}(x_1+h_1, \cdots, x_n+h_n, x_{n+1}+\theta h_{n+1})\;h_{n+1},\quad\theta\in (0,1).$$
    And we write $K_1$ as
    $$ K_1 = \parfrac{f}{x_{n+1}}(x_1+h_1, \cdots, x_n+h_n, x_{n+1})\;h_{n+1} + \beta_{n+1}(\pmb{h}) h_{n+1}. $$
    and 
    $\beta_{n+1}(\pmb{h}) = \parfrac{f}{x_{n+1}}(x_1+h_1, \cdots, x_n+h_n, x_{n+1}+\theta h_{n+1}) - \parfrac{f}{x_{n+1}}(x_1+h_1, \cdots, x_n+h_n, x_{n+1})$.
    Let $\pmb{h}\rightarrow \pmb{0}$ we have $\beta_{n+1}(\pmb{h})\rightarrow 0$, and we complete the proof after combining $K_1$ and $K_2$.
\end{Th}

\begin{Th}{Ex5.2.4.1 (the converse of Th5.2.4)}
    Prove that the converse of Th \{, ID: 5.2.4\} is not true.
    \tcblower
    \textit{Solution}: A counterexample is
    $$ f(x,y) = \begin{cases}
        (x^2+y^2)\sin\frac{1}{x^2+y^2},\quad &(x,y)\neq (0,0)\\
        \;0, \quad &(x,y) = (0,0)
    \end{cases}
    $$
    whose $\parfrac{f}{x}$ is discontinuous at $(0,0)$.
\end{Th}

\begin{Th}{Th5.2.5 (differentiable $\Rightarrow$ all directional derivatives exist)}
    Suppose $f$ is an $n$-real function. If $f$ is differentiable at $\pmb{x}_0$, then the directional derivative of $f$ at $\pmb{x}_0$ in every direction $\pmb{u} = (u_1, \cdots, u_n)^\mathrm{T}$ exist, and
    $$ \parfrac{f}{\pmb{u}}(\pmb{x}_0) = \sum_{i=1}^{n} \parfrac{f}{x_i}(\pmb{x}_0)\,u_i. $$
    \tcblower
    \textit{Pf}: Obvious.
\end{Th}

\begin{Df}{Df5.2.6 (differential of $n$-real $m$-functions)}
    Suppose $\pmb{f}$ is an $n$-real $m$-function and $\pmb{x}_0\in (\text{dom}(f))^\circ$. If there exists some $\pmb{A}\in\mathbb{R}^{m,n}$ s.t. the differential equality
    $$ \pmb{f}(\pmb{x}_0 + \pmb{h}) - \pmb{f}(\pmb{x}_0) = \pmb{Ah} + \pmb{R}(\pmb{h}) $$
    (where 
    $$\lim_{\pmb{h}\to \pmb{0}}\frac{\pmb{R}(\pmb{h})}{\Vert \pmb{h}\Vert} = \pmb{0} $$
    ) holds for $\pmb{h}$ in some $B_\delta(\pmb{0})$. Then the linear function $\pmb{Ah}$ of $\pmb{h}$ is called the \textbf{differential} of $\pmb{f}$ at $\pmb{x}_0$, denoted by $\dif \pmb{f}(\pmb{x}_0)$, and $\pmb{f}$ is said to be differentiable at $\pmb{x}_0$.
\end{Df}

\begin{Rmk}{}
    This definition is merely an extension of Df \{, ID: 5.2.1\}, and so are some of the following definitions and theorems.
    \begin{compactenum}
        \item \textcolor{Th}{(Uniqueness of differential)}
        \item \textcolor{Th}{(Differential by components) Let $\pmb{f} = (f_1, \cdots, f_m)$ be an $n$-real $m$-function and $\pmb{x}_0\in (\text{dom}(f))^\circ$. Then $\pmb{f}$ is differentiable at $\pmb{x}_0$ iff each $f_i$ is differentiable at $\pmb{x}_0$.}
        \item \textcolor{Th}{$\dif \pmb{f}(\pmb{x}_0) = \pmb{Jf}(\pmb{x}_0)\,\pmb{h}$,} \textcolor{Df}{where $\pmb{Jf}(\pmb{x}_0)$ denotes the \textbf{Jacobian matrix} of $\pmb{f}$ at $\pmb{x}_0$: (let $\pmb{f} = (f_1, \cdots, f_m)$ and $\pmb{x} = (x_1, \cdots, x_n)$)
        $$ \pmb{Jf}(\pmb{x}_0) = \begin{bmatrix}
            \parfrac{f_1}{x_1}(\pmb{x}_0) & \cdots & \parfrac{f_1}{x_n}(\pmb{x}_0)\\
            \vdots & \ddots & \vdots\\
            \parfrac{f_m}{x_1}(\pmb{x}_0) & \cdots & \parfrac{f_m}{x_n}(\pmb{x}_0)
        \end{bmatrix}
        $$}
        \item \textcolor{Th}{We certainly have the arithmetics for $\pmb{Jf}(\pmb{x}_0)$ and $\dif\pmb{f}(\pmb{x}_0)$ (still for each equality, the left side can be computed if the right side can, and the left equals right):
        \begin{compactenum}
            \item $\pmb{J}(\pmb{f\pm g})(\pmb{x}_0) = \pmb{Jf}(\pmb{x}_0)\pm\pmb{Jg}(\pmb{x}_0)$;
            \item $\pmb{J}(c\pmb{f})(\pmb{x}_0) = c\,\pmb{Jf}(\pmb{x}_0)$, where $c\in\mathbb{R}$;
            \item $\dif(\pmb{f}\pm\pmb{g})(\pmb{x}_0) = \dif \pmb{f}(\pmb{x}_0)\pm\dif \pmb{g}(\pmb{x}_0)$;
            \item $\dif(c\pmb{f})(\pmb{x}_0) = c\dif \pmb{f}(\pmb{x}_0)$, where $c\in\mathbb{R}$.
        \end{compactenum}}
        \item \textcolor{Th}{(Differentiable $\Rightarrow$ continuous)}
        \item \textcolor{Th}{Suppose $\pmb{f}$ is an $n$-real $m$-function. If $\pmb{Jf}(\pmb{x})$ exists for $\pmb{x}$ in some neighborhood of $\pmb{x}_0$, and $\pmb{Jf}(\cdot)$ is continuous at $\pmb{x}_0$, then $\pmb{f}$ is differentiable at $\pmb{x}_0$.}
        \item The condition 
        $$ \lim_{\pmb{h}\to \pmb{0}}\frac{\pmb{R}(\pmb{h})}{\Vert \pmb{h}\Vert} = \pmb{0} $$
        is equivalent to 
        $$ \lim_{\pmb{h}\to \pmb{0}}\frac{\Vert \pmb{R}(\pmb{h})\Vert}{\Vert \pmb{h}\Vert} = 0, $$
        and the latter is often more convenient to use since we have more arithmetic tools to deal with the norm compared to the vector.
    \end{compactenum}
\end{Rmk}

\begin{Df}{Df5.2.7 ($\pmb{f}\in\mathcal{C}(D)$, $\pmb{f}\in\mathcal{C}^1(D)$)}
    Suppose $D\subseteq\mathbb{R}^n$ is open, and $\pmb{f}: D\rightarrow\mathbb{R}^m$. If $\pmb{f}$ is continuous on $D$, then we denote $\pmb{f}\in\mathcal{C}(D)$; if $\pmb{Jf}(\cdot)$ is continuous on $D$ \textcolor{Th}{(which implies that $\pmb{f}$ is differentiable at every point of $D$)}, then we say that $\pmb{f}$ is \textbf{continuously differentiable}, denoted by $\pmb{f}\in\mathcal{C}^1(D)$.
\end{Df}

\begin{Rmk}{}
    \textcolor{Th}{$\pmb{f}\in\mathcal{C}^1(D) \Rightarrow \pmb{f}\in\mathcal{C}(D)$.}
\end{Rmk}

\begin{Df}{Df5.2.8.-1 (norm of a matrix)}
    Suppose $\mathbb{F}$ is a number field and $m, n\in\mathbb{N}^\ast$. In the matrix space $\mathbb{F}^{m,n}$, consider every matrix as an tuple $m\times n$ long, then the inner product of two matrices $\pmb{A}$ and $\pmb{B}$ is defined as the inner product of the two corresponding tuples, (i.e. $\langle\pmb{A}, \pmb{B}\rangle \triangleq \sum_{(i,j)} a_{(i,j)} b_{(i,j)}$) and the norm of $\pmb{A}$ is defined accordingly as the positive square root of the inner product. The norm of a matrix $\pmb{A}$ is denoted by $\Vert \pmb{A}\Vert$.
\end{Df}

\begin{Th}{Th5.2.8.-1.1 (basic properties of $\Vert\pmb{A}\Vert$)}
    Suppose $\pmb{A}, \pmb{B}$ are matrices over $\mathbb{R}$ that suit the following operations respectively. Then
    \begin{compactenum}
        \item $\Vert \pmb{A} + \pmb{B}\Vert \leq \Vert \pmb{A}\Vert + \Vert \pmb{B}\Vert$.
        \item $\Vert \pmb{A}\pmb{B}\Vert \leq \Vert \pmb{A}\Vert\Vert \pmb{B}\Vert$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: Only prove the second one. Let $\pmb{A} = (a_{ij})$, $\pmb{B} = (b_{ij})$ and $\pmb{AB} = (c_{ij})$. Then
    $$ 
    \begin{aligned}
        & \Vert \pmb{AB}\Vert^2 = \sum_i\sum_j c_{ij}^2 = \sum_i\sum_j\left(\sum_k a_{ik}b_{kj}\right)^2 \\
        & \leq \sum_i\sum_j\left(\sum_k a_{ik}^2\right)\left(\sum_k b_{kj}^2\right) \\
        & = \left( \sum_i\sum_k a_{ik}^2 \right)\left( \sum_j\sum_k b_{kj}^2 \right) = \Vert \pmb{A}\Vert^2\Vert \pmb{B}\Vert^2.
    \end{aligned}
    $$
    where the inequality is just the Cauchy-Schwarz inequality. Hence \textcolor{Th}{$\Vert \pmb{AB}\Vert = \Vert \pmb{A}\Vert\Vert \pmb{B}\Vert$ iff each row vector of $\pmb{A}$ is parallel to each column vector of $\pmb{B}$.}
\end{Th}

\begin{Th}{Th5.2.8 (chain-rule of Jacobian matrix)}
    Suppose $p$-real $n$-function $\pmb{g}$ is differentiable at $\pmb{x}_0$, and $n$-real $m$-function $\pmb{f}$ is differentiable at $\pmb{g}(\pmb{x}_0)$. Then their composition $\pmb{f}\circ\pmb{g}$ ($\pmb{f}\circ\pmb{g}$ may be not composable overall, but it is composable on some $B(\pmb{x}_0)$) is differentiable at $\pmb{x}_0$, and
    $$ \pmb{J}(\pmb{f}\circ\pmb{g})(\pmb{x}_0) = \pmb{Jf}(\pmb{g}(\pmb{x}_0))\;\pmb{Jg}(\pmb{x}_0). $$
    \tcblower
    \textit{Pf}: First we write down the differential equalities:
    $$ 
    \begin{aligned}
        \pmb{f}(\pmb{g}(\pmb{x_0}) + \pmb{l}) &- \pmb{f}(\pmb{g}(\pmb{x_0})) = \pmb{A\,l} + \pmb{R}_1(\pmb{l}), \quad\;\; \pmb{R}_1(\pmb{l})/\Vert\pmb{l}\Vert\rightarrow\pmb{0},\\
        \pmb{g}(\pmb{x}_0 + \pmb{h}) &- \pmb{g}(\pmb{x}_0) = \pmb{B\,h} + \pmb{R}_2(\pmb{h}), \quad \pmb{R}_2(\pmb{h})/\Vert\pmb{h}\Vert\rightarrow\pmb{0}.
    \end{aligned}
    $$
    Then
    $$ 
    \begin{aligned}
        & \pmb{f}(\pmb{g}(\pmb{x}_0 + \pmb{h})) - \pmb{f}(\pmb{g}(\pmb{x}_0)) = \pmb{f}(\pmb{g}(\pmb{x}_0) + \pmb{l}) - \pmb{f}(\pmb{g}(\pmb{x}_0)) \\
        = & \pmb{A\,l} + \pmb{R}_1(\pmb{l}) = \pmb{A}(\pmb{g}(\pmb{x}_0 + \pmb{h}) - \pmb{g}(\pmb{x}_0)) + \pmb{R}_1(\pmb{l}) \\
        = & \pmb{A}(\pmb{B\,h} + \pmb{R}_2(\pmb{h})) + \pmb{R}_1(\pmb{l}) = \pmb{AB\,h} + \pmb{A\,R}_2(\pmb{h}) + \pmb{R}_1(\pmb{l}).
    \end{aligned}
    $$
    And we only need to show that 
    $$ \lim_{\pmb{h}\to\pmb{0}}\frac{\Vert\pmb{A\,R}_2(\pmb{h}) + \pmb{R}_1(\pmb{l})\Vert}{\Vert\pmb{h}\Vert} = \pmb{0}, $$
    and since $\pmb{R}_2(\pmb{h})/\Vert\pmb{h}\Vert\rightarrow\pmb{0}$, we only need to show that
    \begin{equation}
        \lim\limits_{\pmb{h}\to\pmb{0}}\frac{\Vert\pmb{R}_1(\pmb{l})\Vert}{\Vert\pmb{h}\Vert} = \lim\limits_{\pmb{h}\to\pmb{0}}\frac{\Vert\pmb{R}_1(\pmb{Bh} + \pmb{R}_2(\pmb{h}))\Vert}{\Vert\pmb{h}\Vert} = \pmb{0}.
        \tag{1}
    \end{equation}
    Now let $\pmb{\alpha}(\cdot)$ and $\pmb{\beta}(\cdot)$ be the functions s.t. 
    $$ \pmb{R}_1(\pmb{l}) = \pmb{\alpha}(\pmb{l})\,\pmb{l},\quad \pmb{R}_2(\pmb{h}) = \pmb{\beta}(\pmb{h})\pmb{h}, $$
    here $\pmb{\alpha} = (\alpha_1, \cdots, \alpha_m)$ where each $\alpha_i(\cdot)$ is the function constructed as in Th \{, ID: 5.2.3\} (and so is $\pmb{\beta}$). 
    then the equation (1) becomes
    $$ \lim\limits_{\pmb{h}\to\pmb{0}}\frac{\Vert\pmb{\alpha}([\pmb{B} + \pmb{\beta}(\pmb{h})]\pmb{h})\Vert \cdot \Vert [\pmb{B} + \pmb{\beta}(\pmb{h})]\pmb{h}\Vert}{\Vert\pmb{h}\Vert} = \pmb{0}. $$
    And we can prove it by
    $$ 
    \begin{aligned}
        & \frac{\Vert\pmb{\alpha}([\pmb{B} + \pmb{\beta}(\pmb{h})]\pmb{h})\Vert \cdot \Vert [\pmb{B} + \pmb{\beta}(\pmb{h})]\pmb{h}\Vert}{\Vert\pmb{h}\Vert} \\
        \leq & \Vert\pmb{\alpha}([\pmb{B} + \pmb{\beta}(\pmb{h})]\pmb{h})\Vert \cdot \frac{\Vert \pmb{B} + \pmb{\beta}(\pmb{h})\Vert \cdot \Vert\pmb{h}\Vert}{\Vert\pmb{h}\Vert} \\
        = & \Vert\pmb{\alpha}([\pmb{B} + \pmb{\beta}(\pmb{h})]\pmb{h})\Vert \cdot \Vert \pmb{B} + \pmb{\beta}(\pmb{h})\Vert \rightarrow 0.
    \end{aligned}
    $$
    where the last step ``$\rightarrow 0$'' is guaranteed by the continuity of $\pmb{\alpha}$ and $\pmb{\beta}$ at $\pmb{0}$.
\end{Th}

\begin{Th}{Th5.2.8.1 (chain-rules of $\mathcal{C}$, $\mathcal{C}^1$)}
    \begin{compactenum}
        \item \textcolor{Df}{Suppose $\pmb{x}_0\in\mathbb{R}^n$. Then we term the adverbial modifier ``on some neighborhood of $\pmb{x}_0$'' (or ``on $B(\pmb{x}_0)$'', as they are equivalent) ``at $\pmb{x}_0$ locally'' for short.} For example, ``$\pmb{f}\in\mathcal{C}$ at $\pmb{x}_0$ locally'' means that for some neighborhood $D$ of $\pmb{x}_0$, $\pmb{f}\in\mathcal{C}(D)$.
        \item Suppose $\pmb{g}$ is a $p$-real $n$-function and $\pmb{f}$ is an $n$-real $m$-function. Then: \begin{compactenum}
            \item $\pmb{g}\in\mathcal{C}$ at $\pmb{t}_0$ locally and $\pmb{f}\in\mathcal{C}$ at $\pmb{x}_0 = \pmb{g}(\pmb{t}_0)$ locally $\Rightarrow$ $\pmb{f}\circ\pmb{g}\in\mathcal{C}$ at $\pmb{t}_0$ locally.
            \item $\pmb{g}\in\mathcal{C}^1$ at $\pmb{t}_0$ locally and $\pmb{f}\in\mathcal{C}^1$ at $\pmb{x}_0 = \pmb{g}(\pmb{t}_0)$ locally $\Rightarrow$ $\pmb{f}\circ\pmb{g}\in\mathcal{C}^1$ at $\pmb{t}_0$ locally.
        \end{compactenum}
    \end{compactenum}
    \tcblower
    \textit{Pf}: Obvious.
\end{Th}

\begin{Rmk}{}
    This theorem greatly simplifies the discussion of the differentiability of composite functions in future, expecially in the proof of the hidden function theorem.
\end{Rmk}

\begin{Df}{Df5.2.9.-1}
    Denote the Jacobian matrix of an $n$-real $m$-function $\pmb{f}$ as $\parfrac{\pmb{f}}{\pmb{x}}$ or $\parfrac{\pmb{y}}{\pmb{x}}$ if we write $\pmb{f}$ as $\pmb{y} = \pmb{f}(\pmb{x})$. Also, denote $\dif\pmb{f}$ as $\dif\pmb{y}$.
\end{Df}

\begin{Th}{Th5.2.9 (invariant form of one-order differential)}
    Let $\pmb{y} = \pmb{f}(\pmb{x})$. Then the differential of $\pmb{f}$ is
    \begin{equation}
        \dif\pmb{y} = \parfrac{\pmb{y}}{\pmb{x}}\dif\pmb{x}
        \tag{1}
    \end{equation}
    where $\dif\pmb{x}$ is the differential of the identity function $\pmb{x}$ about $\pmb{x}$.
    If further $\pmb{x} = \pmb{g}(\pmb{t})$ so that for $\pmb{y} = \pmb{f}(\pmb{g}(\pmb{t}))$ can be applied the Th \{, ID: 5.2.8\}, then
    $$ \dif\pmb{x} = \parfrac{\pmb{x}}{\pmb{t}}\dif\pmb{t}, $$
    and we have
    $$ \dif\pmb{y} = \parfrac{\pmb{y}}{\pmb{t}}\dif\pmb{t} = \parfrac{\pmb{y}}{\pmb{x}}\parfrac{\pmb{x}}{\pmb{t}}\dif\pmb{t} = \parfrac{\pmb{y}}{\pmb{x}}\dif\pmb{x}. $$
    This implies that the equation (1) holds no matter whether $\pmb{x}$ is a independent variable or a dependent variable, \textcolor{Df}{which is called the invariant form of one-order differential.}
    \tcblower
    \textit{Pf}: Obvious.
\end{Th}

\begin{Th}{Th5.3.1 (theorem of implicit function)}
    Suppose $F$ is a 2-real function and $(x_0, y_0)\in (\text{dom}(F))^\circ$. If
    \begin{compactenum}
        \item $F\in\mathcal{C}^1$ at $(x_0, y_0)$ locally;
        \item $F(x_0, y_0) = 0$;
        \item $\parfrac{F}{y}(x_0, y_0)\neq 0$;
    \end{compactenum}
    then there is some open rectangle $I\times J$ (``open rectangle'' means it is an open interval in each dimension) containing $(x_0, y_0)$ s.t.
    \begin{compactenum}
        \item[(i)] $\forall x\in I$, $\exists!\, f(x)\in J$ s.t. $F(x, f(x)) = 0$ (and hence we can define the function $f: I\rightarrow J$);
        \item[(ii)] $f\in\mathcal{C}^1(I)$;
        \item[(iii)] For any $x\in I$,
        \begin{equation}
            f^\prime (x) = -\frac{\parfrac{F}{x}(x, y)}{\parfrac{F}{y}(x, y)}
            \tag{1}
        \end{equation}
        where $y=f(x)$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: First we prove the existence of $f$. Since $\parfrac{F}{y}$ is continuous at $(x_0, y_0)$ locally, and $\parfrac{F}{y}(x_0, y_0)\neq 0$, say, $\parfrac{F}{y}(x_0, y_0) > 0$, we can find an open rectangle $I_1\times J$ containing $(x_0, y_0)$ where $\parfrac{F}{y}(x, y) > 0$. 
    Let $J = (c, d)$, then fix $x\in I$, the function $g(y) = F(x, y)$ defined on $y\in [c,d]$ (assumed $g$ is continuous at $c$ and $d$, otherwise we can shrink $(c,d)$ a little bit into $(c^\prime, d^\prime)$ so that it is continuous at $c$ and $d$) is interval-derivable, and thus is increasing since $g^\prime = \parfrac{F}{y} > 0$. Hence
    $$ F(x_0,c) < F(x_0, y_0) = 0 < F(x_0,d). $$
    Also due to continuity, we can find an open interval $I$ containing $x_0$ s.t. $ F(x,c) < 0 < F(x,d) $ for any $x\in I$, so that we can find a unique $f(x)\in (c,d)$ s.t. $F(x, f(x))=0$ and thus $I\times J$ is what we want.\\
    Then we prove that $f\in\mathcal{C}^1 (I)$. We must first prove that $f$ is continuous on $I$, especially, at $x_0$. From the analysis above, we can find that no matter how narrow $J$ is, we can always find the desired $I$, which indicates that $f$ is continuous at $x_0$. Secondly, for any $x_1\in I$, repeat the analysis above to get a function $f_1$ s.t. $F(x, f_1(x)) = 0$ and $f_1$ is continuous at $x_1$. Since the uniqueness of $f$, $f_1$ must be $f$, so that $f$ is continuous at $x_1$.\\
    At $(x_0, y_0)$ locally,
    $$ F(x_0+h, y_0+k) = F(x_0+h, y_0+k) - F(x_0, y_0) = Ah + Bk + \alpha h+\beta k $$
    where $A$, $B$ are the corresponding partial derivatives, $\alpha = \alpha (h,k)$, $\beta = \beta (h,k)$ and $\alpha, \beta\rightarrow 0$ when $(h,k)\rightarrow\pmb{0}$. Then let $k = f(x_0+h)-f(x_0)$ to rewrite the equation:
    $$ 0 = F(x_0+h, f(x_0+h)) = Ah + B[f(x_0+h)-f(x_0)] + \alpha h+\beta [f(x_0+h)-f(x_0)] $$
    where $\alpha = \alpha (h, f(x_0+h)-f(x_0))\rightarrow 0$ as $h\rightarrow 0$, and so does $\beta$. Then we can prove (ii) and (iii) by dividing the equation by $h$ and then taking $h\rightarrow 0$.
\end{Th}

\begin{Rmk}{}
    The formula (1) needs no rote, as it can be easily derived by composite differentiating to the equation $F(x,f(x)) = 0$.
\end{Rmk}

\begin{Th}{Th5.3.1.1 (theorem of implicit function) (extension of Th \{, ID: 5.3.1\})}
    Suppose $F$ is a $(n+1)$-real function ($n\geq 1$) and $(\pmb{x}_0, y_0)\in (\text{dom}(F))^\circ$. If
    \begin{compactenum}
        \item $F\in\mathcal{C}^1$ at $(\pmb{x}_0, y_0)$ locally;
        \item $F(\pmb{x}_0, y_0) = 0$;
        \item $\parfrac{F}{y}(\pmb{x}_0, y_0)\neq 0$;
    \end{compactenum}
    then there is some open rectangle $\pmb{I}\times J$ ($\pmb{I}\subseteq\mathbb{R}^n$) containing $(\pmb{x}_0, y_0)$ s.t.
    \begin{compactenum}
        \item[(i)] $\forall \pmb{x}\in \pmb{I}$, $\exists!\, f(\pmb{x})\in J$ s.t. $F(\pmb{x}, f(\pmb{x})) = 0$ (and hence we can define the function $f: \pmb{I}\rightarrow J$);
        \item[(ii)] $f\in\mathcal{C}^1(\pmb{I})$;
        \item[(iii)] For any $\pmb{x}\in\pmb{I}$,
        $$ \parfrac{f}{x_i}(\pmb{x}) = -\frac{\parfrac{F}{x_i}(\pmb{x}, y)}{\parfrac{F}{y}(\pmb{x}, y)}, \quad (i=1, \cdots, n)$$
        where $y=f(\pmb{x})$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: As what we have done in Th \{, ID: 5.3.1\}.
\end{Th}

\begin{Rmk}{}
    This can be further extended to the theorem of implicit map (see the next theorem). Before state the theorem, we first claim this notation: \textcolor{Df}{Consider the $(n+m)$-real $m$-function ($n,m\geq 1$) $\pmb{F}$, written as $\pmb{F}(\pmb{x}, \pmb{y})$ ($\pmb{x}\in\mathbb{R}^n$, $\pmb{y}\in\mathbb{R}^m$). Then block $\pmb{JF}$ as $\pmb{J F} = \big[\pmb{J_x F}\;\; \pmb{J_y F}\big]$, where
    $$ 
    \pmb{J_x F} = \begin{bmatrix}
        \parfrac{F_1}{x_1} & \cdots & \parfrac{F_1}{x_n} \\
        \vdots & & \vdots \\
        \parfrac{F_m}{x_1} & \cdots & \parfrac{F_m}{x_n}
    \end{bmatrix}
    \quad\text{ and }\quad
    \pmb{J_y F} = \begin{bmatrix}
        \parfrac{F_1}{y_1} & \cdots & \parfrac{F_1}{y_m} \\
        \vdots & \ddots & \vdots \\
        \parfrac{F_m}{y_1} & \cdots & \parfrac{F_m}{y_m}
    \end{bmatrix}.
    $$
    }
\end{Rmk}

\begin{Th}{Th5.3.1.2 (theorem of implicit function) (extension of Th \{, ID: 5.3.1.1\})}
    Suppose $\pmb{F}$ is an $(n+m)$-real $m$-function ($n,m\geq 1$) and $(\pmb{x}_0, \pmb{y}_0)\in (\text{dom}(\pmb{F}))^\circ$. If
    \begin{compactenum}
        \item $\pmb{F}\in\mathcal{C}^1$ at $(\pmb{x}_0, \pmb{y}_0)$ locally;
        \item $\pmb{F}(\pmb{x}_0, \pmb{y}_0) = \pmb{0}$;
        \item $\det \pmb{J}_{\pmb{y}} \pmb{F}(\pmb{x}_0, \pmb{y}_0)\neq 0$;
    \end{compactenum}
    then there is some open rectangle $\pmb{I}\times\pmb{J}$ ($\pmb{I}\subseteq\mathbb{R}^n$, $\pmb{J}\subseteq\mathbb{R}^m$) containing $(\pmb{x}_0, \pmb{y}_0)$ s.t.
    \begin{compactenum}
        \item[(i)] $\forall \pmb{x}\in \pmb{I}$, $\exists!\, \pmb{f}(\pmb{x})\in J$ s.t. $\pmb{F}(\pmb{x}, \pmb{f}(\pmb{x})) = 0$ (and hence we can define the function $\pmb{f}: \pmb{I}\rightarrow \pmb{J}$);
        \item[(ii)] $\pmb{f}\in\mathcal{C}^1(\pmb{I})$;
        \item[(iii)] For any $\pmb{x}\in\pmb{I}$,
        $$ \pmb{Jf}(\pmb{x}) = -[\pmb{J}_{\pmb{y}} \pmb{F}(\pmb{x}, \pmb{y})]^{-1} \pmb{J}_{\pmb{x}} \pmb{F}(\pmb{x}, \pmb{y}), $$
        where $\pmb{y} = \pmb{f}(\pmb{x})$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: Omit here, and we leave it to the exercise file so that you can find the answer from the source.
\end{Th}

\begin{Th}{Th5.3.2 (theorem of local inverse function)}
    Suppose $D\subseteq\mathbb{R}^n$ is open and $\pmb{f}:D\rightarrow\mathbb{R}^n$. If
    \begin{compactenum}
        \item $\pmb{f}\in\mathcal{C}^1(D)$;
        \item There is some $\pmb{x}_0\in D$ s.t. $\det \pmb{Jf}(\pmb{x}_0)\neq 0$;
    \end{compactenum}
    then there are a neighborhood $U$ of $\pmb{x}_0$ and a neighborhood $V$ of $\pmb{y}_0$ s.t.
    \begin{compactenum}
        \item[(i)] $\pmb{f}[U] = V$, and $\pmb{f}: U\rightarrow V$ (the restriction of $\pmb{f}$ to $U$) is bijective (and thus $\pmb{f}$ has an inverse function $\pmb{f}^{-1}: V\rightarrow U$);
        \item[(ii)] $\pmb{f}^{-1}\in\mathcal{C}^1(V)$;
        \item[(iii)] For any $\pmb{y}\in V$,
        $$ \pmb{Jf}^{-1}(\pmb{y}) = \left(\pmb{Jf}(\pmb{x})\right)^{-1}, $$
        where $\pmb{x} = \pmb{f}^{-1}(\pmb{y})$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: Just apply the theorem of implicit function to $\pmb{F}(\pmb{y}, \pmb{x}) = \pmb{f}(\pmb{x})-\pmb{y}$.
\end{Th}

\begin{Th}{Th5.3.2.1 (theorem of inverse function)}
    Suppose $D\subseteq\mathbb{R}^n$ is open and $\pmb{f}:D\rightarrow\mathbb{R}^n$. If
    \begin{compactenum}
        \item $\pmb{f}\in\mathcal{C}^1(D)$;
        \item $\det \pmb{Jf}(\pmb{x})\neq 0$ for any $\pmb{x}\in D$ (and thus $E\triangleq\pmb{f}[D]$ is open);
        \item $\pmb{f}$ is injective (and thus $\pmb{f}$ has an inverse funcion $\pmb{f}^{-1}: E\rightarrow D$);
    \end{compactenum}
    then
    \begin{compactenum}
        \item[(i)] $\pmb{f}^{-1}\in\mathcal{C}^1(E)$;
        \item[(ii)] For any $\pmb{y}\in V$,
        $$ \pmb{Jf}^{-1}(\pmb{y}) = \left(\pmb{Jf}(\pmb{x})\right)^{-1}, $$
        where $\pmb{x} = \pmb{f}^{-1}(\pmb{y})$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: First we prove that $E$ is open. Since we can apply the theorem of local inverse function to each point $\pmb{x}$ in $D$, we then find the local inverse function $\pmb{f}_{\pmb{x}_0}^{-1}: V_{\pmb{x}_0}\rightarrow U_{\pmb{x}_0}$ for each $\pmb{x}_0$, where $U_{\pmb{x}_0}$ and $V_{\pmb{x}_0}$ are open. Then $E = \bigcup_{\pmb{x}\in D}\pmb{f}[U_{\pmb{x}}] = \bigcup_{\pmb{x}\in D} V_{\pmb{x}}$ is open. \\
    Since $\pmb{f}^{-1}$ coincides with the local inverse function $\pmb{f}_{\pmb{x}}^{-1}$ locally, we have (i) and (ii).
\end{Th}

\begin{Df}{Df5.4 (partial derivatives of higher orders)}
    For an $n$-real function $f$, if the partial derivatives have their own partial derivatives, then it yields the partial derivatives of $f$ of a higher order. For example, for $f(x,y,z)$, each of the following partial derivatives
    $$ \biparfrac{f}{\partial x^2} \triangleq \parfrac{}{x}\left(\parfrac{f}{x}\right), \quad \biparfrac{f}{\partial y\,\partial x} \triangleq \parfrac{}{y}\left(\parfrac{f}{x}\right) $$
    is called a \textbf{2-order partial derivative} of $f$.
\end{Df}

\begin{Rmk}{}
    \textcolor{Df}{A higher-order partial derivative that involves different variables (e.g. $\biparfrac{f}{\partial y\,\partial x}$) is called a \textbf{mixed partial derivative}. (For those that involve the same variable, let us call them \textbf{pure partial derivatives}.)} \textcolor{Df}{Also we have the term ``\textbf{partial derivative function of higher orders}''.}
    In most of the time, the order of variables in the mixed partial derivative does not matter. \textcolor{Th}{But here is a typical counterexample:
    $$ f(x,y) = \begin{cases}
        xy\frac{x^2-y^2}{x^2+y^2},\quad & (x,y)\neq\pmb (0,0)\\
        0, & (x,y) = (0,0)
    \end{cases}
    $$
    for which $\biparfrac{f}{\partial y\,\partial x}(0,0)\neq\biparfrac{f}{\partial x\,\partial y}(0,0)$.}
    The next theorem proposes a sufficient condition for the mixed derivatives to be independent of the order of the variables.
\end{Rmk}

\begin{Th}{Th5.4.1 (symmetry of 2-order mixed derivatives)}
    Suppose $f$ is a $2$-real function written as $f(x,y)$. If:
    \begin{compactenum}
        \item $\biparfrac{f}{\partial y\,\partial x}$ exists on some neighborhood of $(x_0, y_0)$;
        \item $\biparfrac{f}{\partial y\,\partial x}$ is continuous at $(x_0, y_0)$;
        \item $\parfrac{f}{y}$ exists on some neighborhood of $(x_0, y_0)$;
    \end{compactenum}
    then $\biparfrac{f}{\partial x\,\partial y}(x_0, y_0)$ exists and 
    $$ \biparfrac{f}{\partial x\,\partial y}(x_0, y_0) = \biparfrac{f}{\partial y\,\partial x}(x_0, y_0). $$
    \tcblower
    \textit{Pf}: Let us assume that $(x_0, y_0) = (0,0)$. Then denote $F(x,y) = f(x,y)-f(x,0)-f(0,y)+f(0,0)$ and $g(t) = f(t,y) - f(t,0)$. According to the Lagrange intermediate-value theorem,
    $$ \begin{aligned}
        F(x,y) &= g(x) - g(0) = g^\prime (\theta_1 x) x \\
        &= \left(\parfrac{f}{x}(\theta_1 x, y) - \parfrac{f}{x}(\theta_1x, 0)\right)x \\
        &= \biparfrac{f}{\partial y\,\partial x}(\theta_1 x, \theta_2 y)\, yx
    \end{aligned}
    $$
    where $\theta_1, \theta_2\in (0,1)$.
    Then:
    $$
    \begin{aligned}
        \biparfrac{f}{\partial x\, \partial y}(0,0) &= \lim_{x\to 0}\frac{1}{x}\left(\parfrac{f}{y}(x,0) - \parfrac{f}{y}(0,0)\right) \\
        &= \lim_{x\to 0}\frac{1}{x}\left(\lim_{y\to 0}\frac{f(x,y)-f(x,0)}{y} - \lim_{y\to 0}\frac{f(0,y)-f(0,0)}{y}\right) \\
        &= \lim_{x\to 0}\frac{1}{x}\lim_{y\to 0}\frac{f(x,y)-f(x,0)-f(0,y)+f(0,0)}{y} \\
        &= \lim_{x\to 0}\lim_{y\to 0}\frac{F(x,y)}{xy} = \biparfrac{f}{\partial y\,\partial x}(0,0).
    \end{aligned}
    $$
\end{Th}

\begin{Rmk}{}
    This theorem can be further extended to the cases of $n$-real function $f(x_1, \cdots, x_n)$.\\ \textcolor{Th}{Suppose $f$ is an $n$-real function written as $f(x_1, \cdots, x_n)$, $x_i, x_j$ are two certain variables ($i\neq j$), and $\pmb{x}_0 = (x_{10}, \cdots, x_{n0})$. If:
    \begin{compactenum}
        \item $\biparfrac{f}{\partial x_j\,\partial x_i}$ exists on some neighborhood of $\pmb{x}_0$;
        \item $\biparfrac{f}{\partial x_j\,\partial x_i}$ is continuous at $\pmb{x}_0$;
        \item $\parfrac{f}{x_j}$ exists on some neighborhood of $\pmb{x}_0$;
    \end{compactenum}
    then $\biparfrac{f}{\partial x_i\,\partial x_j}(\pmb{x}_0)$ exists and
    $$ \biparfrac{f}{\partial x_i\,\partial x_j}(\pmb{x}_0) = \biparfrac{f}{\partial x_j\,\partial x_i}(\pmb{x}_0). $$} \\
    Also, we can simplify the theorem with a stronger premise. \textcolor{Th}{Suppose $f: D\rightarrow\mathbb{R}$ and $D\subseteq\mathbb{R}^n$ is open. For two certain variables $x_i, x_j$ ($i\neq j$) in the expression $f(x_1, \cdots, x_n)$, if both $\biparfrac{f}{\partial x_i\,\partial x_j}$ and $\biparfrac{f}{\partial x_j\,\partial x_i}$ are continuous on $D$, then they are equal at any point $\pmb{x}$ in $D$.} \\
    And we would like to further extend this to the mixed partial derivatives of higher orders, in the Th \{, ID: 5.4.2\}. 
\end{Rmk}

\begin{Df}{Df5.4.2.-1 ($\mathcal{C}^m$)}
    Suppose $f$ is an $n$-real function and $D\subseteq\text{dom}(f)$ is open. Then we say $f\in\mathcal{C}^m(D)$ ($m\in\mathbb{N}$) if all the $m$-order partial derivative functions (no matter pure or mixed) are continuous on $D$.
\end{Df}

\begin{Rmk}{}
    Note that $m$ here can be $0$, and $\mathcal{C}^0$ is just $\mathcal{C}$ we defined before. Of course, this definition is an extension of the previous $\mathcal{C}$ and $\mathcal{C}^1$.\\
    Here we just require $D\subseteq\text{dom}(f)$ rather than forcing $f: D\rightarrow\mathbb{R}$, since the simply restriction of $f$ on an subset of its domain keeps all the differentiation behavior of $f$ as long as the subset is open. \\
    \textcolor{Th}{$\mathcal{C}^m(D)$ is a very strong statement, as it implies all of $\mathcal{C}^k$ ($k=0,1,\cdots, m-1$).}
\end{Rmk}

\begin{Th}{Th5.4.2 (symmetry of higher-order partial derivatives)}
    Suppose $f$ is an $n$-real function written as $f(x_1, \cdots, x_n)$. If $f\in\mathcal{C}^m(D)$, then on $D$, all those $k$-order partial derivatives functions ($k\leq m$) are symmetric. (Here the ``symmetric'' means that: if two $k$-order partial derivatives differ only in the sequence of variables, then they are equal at any point in $D$.)
    \tcblower
    \textit{Pf}: Given a higher-order partial derivative function (with the denominator $\partial x_{k_1}\cdots\partial x_{k_l}$), we can arbitrarily swap any adjacent pair of $\partial x_{k_i}$'s according to the Rmk \{, ID: 5.4.1\}, by which we can achieve any sequence of variables.
\end{Th}

\begin{Df}{Df5.4.2.2 (indices-notation of higher-order partial derivatives)}
    \begin{compactenum}
        \item An $n$-tuple $\pmb{\alpha} = (\alpha_1, \cdots, \alpha_n)$ made up of non-negative integers $\alpha_1, \cdots, \alpha_n$ is called a \textbf{multiple index}, and we denote $|\pmb{\alpha}| = \sum_{i=1}^{n} \alpha_i$, $\pmb{\alpha !} = \prod_{i=1}^{n}(\alpha_i!)$.
        \item Suppose $f$ is an $n$-real function written as $f(x_1, \cdots, x_n)$, and $\pmb{x}\in\mathbb{R}^n$. Given a multiple index $\pmb{\alpha} = (\alpha_1, \cdots, \alpha_n)$, then denote 
        $$ \mathrm{D}^{\pmb{\alpha}} f(\pmb{x}) \triangleq \frac{\partial^{\alpha_1+\cdots+\alpha_n} f}{\partial x_1^{\alpha_1}\cdots\partial x_n^{\alpha_n}} (\pmb{x}), $$
        if the symmetry of higher-order partial derivatives holds.
    \end{compactenum}
\end{Df}

\begin{Th}{Ex5.4.2.1 ($\mathcal{P}_\mathbb{F}(x_1,\cdots, x_n)\sim\mathcal{P}(\mathbb{F}^n)$)}
    Prove that two $n$-variate polynomial functions $f$ and $g$ in $\mathcal{P}(\mathbb{R}^n)$ are equal iff their corresponding coefficients are all equal (so that $\mathcal{P}(\mathbb{R}^n)$ and $\mathcal{P}_\mathbb{R}(x_1, \cdots, x_n)$ can be deemed identical (see the Rmk \{, ID: 1.13\})).
    \textit{Pf}: Only need to prove that the $n$-variate polynomial function $f$ has only zero-coefficients if $f$ is zero. Write $f$ as
    $$ f(\pmb{x}) = \sum_{\pmb{\alpha}} \varphi(\pmb{\alpha}) \pmb{x}^{\pmb{\alpha}}, $$
    then we assume that $f(\pmb{x}) = 0$ for any $\pmb{x}$, and try to prove that $\varphi(\pmb{\alpha}) = 0$ for any $\pmb{\alpha}$. Like the unary polynomial functions, we can specifically diminish a term with some certain $\pmb{\alpha}$ via some appropriate ways of partial derivative. Now we clarify this with an example case of $f$:
    $$ f(x,y,z) = ax^2y+bxyz^2+cx^2y^3z = 0 $$
    For the term $ax^2y$, since the powers of $x$ and $y$ are $2$ and $1$ respectively, we consider this mixed partial derivative function of $f$: 
    $$ \frac{\partial^{2+1} f}{\partial x^2\,\partial y}(x,y,z) = 2a+0+6cy^2z = 0 $$
    And we let $x=y=z=0$ here to yield that $a=0$. So forth, we can in the same way prove that $b=c=0$.
\end{Th}

\begin{Rmk}{}
    Actually this theorem can be strengthened: \textcolor{Th}{If $f(\pmb{x})=0$ for any $\pmb{x}\in B$, where $B = B(\pmb{x}_0)$ is an open ball centered at some point $\pmb{x}_0$, then all the coefficients of $f$ are zeros.} Actually to prove this, you can first let $\pmb{x}_0 = \pmb{x}$, and then just copy the proof above.
\end{Rmk}

\begin{Df}{Df5.4.2.3 (Hessian matrix)}
    Suppose $f$ is an $n$-real function and $\pmb{x}_0$. Then
    $$ \pmb{H} f(x_0) \triangleq 
    \begin{bmatrix}
        \biparfrac{f}{\partial x_1^2}(\pmb{x}_0) & \cdots & \biparfrac{f}{\partial x_1 \partial x_n}(\pmb{x}_0) \\
        \vdots & \ddots & \vdots \\
        \biparfrac{f}{\partial x_n \partial x_1}(\pmb{x}_0) & \cdots & \biparfrac{f}{\partial x_n^2}(\pmb{x}_0)
    \end{bmatrix}
    $$
    is called the Hessian matrix of $f$. \textcolor{Th}{If $f\in\mathcal{C}^2(B(x_0))$, then $\pmb{H}f(x_0)$ exists, and it is a symmetric square matrix.}
\end{Df}

\end{document}