\documentclass{article}

    \usepackage{xcolor}
    \definecolor{pf}{rgb}{0.4,0.6,0.4}
    \usepackage[top=1in,bottom=1in, left=0.8in, right=0.8in]{geometry}
    \usepackage{setspace}
    \setstretch{1.2} 
    \setlength{\parindent}{0em}

    \usepackage{paralist}
    \usepackage{cancel}

    \usepackage{ctex}
    \usepackage{amssymb}
    \usepackage{amsmath}

    \usepackage{tcolorbox}
    \definecolor{Df}{RGB}{0, 184, 148}
    \definecolor{Th}{RGB}{9, 132, 227}
    \definecolor{Rmk}{RGB}{215, 215, 219}
    \newtcolorbox{Df}[2][]{colbacktitle=Df, colback=white, title={\large\color{white}#2},fonttitle=\bfseries,#1}
    \newtcolorbox{Th}[2][]{colbacktitle=Th, colback=white, title={\large\color{white}#2},fonttitle=\bfseries,#1}
    \newtcolorbox{Rmk}[2][]{colbacktitle=Rmk, colback=white, title={\large\color{black}{Remarks}},fonttitle=\bfseries,#1}

    \title{\LARGE \textbf{Derivatives}}
    \author{\large Jiawei Hu}

\begin{document}
\maketitle

This is the 3rd chapter of Mathematical Analysis, which is about \textbf{the Derivatives of Functions}. By the way, we now pre-claim some commonly-used notations and terms:
\begin{Df}{Notations and Terms}
    \begin{compactenum}
        \item $\mathbb{C}$: the set of the complex numbers;
        \item $\mathbb{R}$: the set of the real numbers; $\mathbb{R}_\infty = \mathbb{R}\cup\{-\infty, \infty\}$;
        \item $\mathbb{Q}$: the set of the rational numbers;
        \item $\mathbb{Z}$: the set of the integers;
        \item $\mathbb{N}$: the set of the natural numbers;
        \item $\mathbb{N^\ast}$: the set of the positive integers.
        \item $\sideset{^R}{}{\mathop{D}}$: the set of all functions from $D$ to $R$ (with domain $D$ and range in $R$).
        \item An agreement for the length of a list: if we write $a_1, \dots, a_n$, then we indicate that $n$ is finite and that $n\geq 1$; if we write $a_0, \dots, a_n$, then we indicate that $n$ is finite and that $n\geq 0$.
        \item Keep coincident in the notions and notations of functions with the chapter 1 of course 0, including the ones of domain, range, restriction, image, pre-image, inverse and composition. Specifically for a function $f: A\rightarrow B$ and some sets $E\subseteq A$ and $F\subseteq B$, the image of $E$ and the pre-image of $F$ under $f$ are just:
        $$f[E] = \{f(x): x\in E\},\quad f^{-1}[F] = \{x\in A: f(x)\in F\}$$
        \item Since in this course we major in the basic analysis on $\mathbb{R}$, we will use the term ``real function'' to refer to a function $f: A\rightarrow \mathbb{R}$ where $A\subseteq \mathbb{R}$.
        \item $\infty$: positive infinity; $-\infty$: negative infinity; $\pm\infty$: infinity.
        \item For the existence of a limit, if we have used the symbol $\lim\limits_{x\to x_0} f(x)$ in an expression (such as an equality, an inequality or some expressions involving some other numbers), then without explicitly specification, we imply that the limit exists (``exist'' means finite according to the chapter 1).
        \item An interval is a subset of $\mathbb{R}$ of one of the following forms: $(a,b)$, $[a,b]$, $(a,b]$, $[a,b)$, $(a, \infty)$, $(-\infty, b)$, $(-\infty, \infty)$, where $a, b\in\mathbb{R}$ and $a<b$. Please identify whether $(a,b)$ stands for a tuple or an open interval from the context by yourself.
        \item Monotonic function: ``increasing'' for ``$\geq$'', ``strictly increasing'' for ``$>$''.
    \end{compactenum}
\end{Df}

Here is the \textbf{Quick Search} for this chapter:
\begin{Th}{Quick Search}
    \begin{compactdesc}
        \item[] (3.3.*): Basic properties of derivatives (arithmetics, chain-rule, inverse function \dots).
        \item[] (3.5.*): Intermediate-value theorems of derivatives.
        \item[] (3.6.1.*): Monotonicity and derivatives.
        \item[] (3.6.2.*): Extremum and derivatives.
        \item[] (3.6.3.*): Convexity and derivatives.
        \item[] (3.7.*): L'Hospital's rule.
    \end{compactdesc}
\end{Th}

Then with everything prepared, here we go.

\begin{Df}{Df3.1 (Derivatives (导数))}
    Suppose $f$ is a real function well-defined near and at $x_0\in\mathbb{R}$ (the term ``well-defined near and at $x_0$'' means, literally, that there exists some $B_\eta(x_0)$ s.t. $B(\eta)\subseteq \text{dom}(f)$). Then the limit
    $$ \lim_{h\to 0} \frac{f(x_0+h)-f(x_0)}{h} $$
    (if it exists) is called the \textbf{derivative} of $f$ at $x_0$, denoted by $f^\prime(x_0)$, and $f$ is said to be \textbf{derivable (可导的)} at $x_0$. 
\end{Df}

\begin{Rmk}{}
    \begin{compactenum}
        \item \textcolor{Df}{Suppose $f$ is a real function and $x_0\in\mathbb{R}$. If $f$ is not well-defined near and at $x_0$, or it is, but the limit in Definition 3.1 does not exist, then we say that $f$ is not derivable at $x_0$ (or that $f$ has no derivative at $x_0$) (or that the derivative of $f$ at $x_0$ does not exist).}
        \item \textcolor{Df}{Suppose $f$ is a real function and $x_0\in\mathbb{R}$. Suppose also there exists some $r>0$ s.t. $[x_0, x_0+r)\subseteq \text{dom}(f)$ (resp. $(x_0-r, x_0]\subseteq \text{dom}(f)$). Then the limit 
        $$ \lim_{h\to 0^+} \frac{f(x_0+h)-f(x_0)}{h}\quad \left(\text{resp. } \lim_{h\to 0^-} \frac{f(x_0+h)-f(x_0)}{h}\right) $$
        is called the \textbf{right derivative} (resp. \textbf{left derivative}) of $f$ at $x_0$, denoted by $f_+^\prime (x_0)$ (resp. $f_-^\prime (x_0)$) and $f$ is said to be \textbf{right-derivable} (resp. \textbf{left-derivable}) at $x_0$. (Accordingly, define the cases of ``not right-derivable'' and ``not left-derivable'' as above.)} Clearly, \textcolor{Th}{Suppose $f$ is a real function and $x_0, l\in\mathbb{R}$. Then $f^\prime(x_0) = d$ iff $f_+^\prime(x_0) = f_-^\prime(x_0) = d$.}
    \end{compactenum}
\end{Rmk}

\begin{Th}{Df3.2 (derivable and continuous)}
    Suppose $f$ is a real function and $x_0\in\mathbb{R}$. Then 
    $$ f \text{ is derivable at } x_0 \Rightarrow f \text{ is continuous at } x_0. $$
    \tcblower
    \textit{Pf}: Suppose $f$ is derivable at $x_0$. Then the limit
    $$ \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0} $$
    exists, and so does the limit 
    $$\lim\limits_{x\to x_0} (f(x)-f(x_0)) = \lim\limits_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}\cdot \lim\limits_{x\to x_0} (x-x_0) = 0.$$ 
    Thus $\lim\limits_{x\to x_0} f(x) = f(x_0)$, which means $f$ is continuous at $x_0$. 
\end{Th}

\begin{Rmk}{}
    \textcolor{Th}{While differentiability implies continuity, the converse is not true. For example, the function $f(x) = |x|$ is continuous at $x=0$ but not derivable at $x=0$.}
\end{Rmk}

\begin{Th}{Th3.3.1 (arithmics of derivatives)}
    Suppose real functions $f$ and $g$ are both derivable at $x_0\in\mathbb{R}$. Then:
    \begin{compactenum}
        \item $(f\pm g)^\prime(x_0) = f^\prime(x_0) \pm g^\prime(x_0)$;
        \item $(fg)^\prime(x_0) = f^\prime(x_0)g(x_0) + f(x_0)g^\prime(x_0)$;
        \item $(f/g)^\prime(x_0) = \frac{f^\prime(x_0)g(x_0) - f(x_0)g^\prime(x_0)}{[g(x_0)]^2}$ (provided $g(x_0)\neq 0$).
    \end{compactenum}
    \tcblower
    \textit{Pf}:
    \begin{compactenum}
        \item Trivial.
        \item Take the trick that ``add and subtract the same term'':
        $$
        \begin{aligned}
            & \frac{f(x)g(x) - f(x_0)g(x_0)}{x-x_0} = \frac{f(x)g(x) - f(x_0)g(x) + f(x_0)g(x) - f(x_0)g(x_0)}{x-x_0} \\
            &= \frac{f(x)(g(x)-g(x_0)) + g(x_0)(f(x)-f(x_0))}{x-x_0} \\
            &= f(x)\frac{g(x)-g(x_0)}{x-x_0} + g(x_0)\frac{f(x)-f(x_0)}{x-x_0}.
        \end{aligned}
        $$
        \item First derive the derivative of $1/g$.
    \end{compactenum}
\end{Th}

\begin{Th}{Th3.3.2 (chain-rule)}
    Suppose real functions $\varphi$ is derivable at $t_0\in\mathbb{R}$ and $f$ is derivable at $x_0 = \varphi(t_0)$. Then the composite function $f\circ\varphi$ ($f\circ\varphi$ may be not composable overall, but it is composable near and at $x_0$) is derivable at $t_0$ and 
    $$ (f\circ\varphi)^\prime(t_0) = f^\prime(x_0)\varphi^\prime(t_0). $$
    \tcblower
    \textit{Pf}: $$ \frac{f(\varphi(t))-f(\varphi(t_0))}{t-t_0} = \frac{f(\varphi(t))-f(\varphi(t_0))}{\varphi(t)-\varphi(t_0)}\cdot \frac{\varphi(t)-\varphi(t_0)}{t-t_0}. $$
    And then take the limit. In this limit of composite functions, the outer function is
    $$ g(x) = \left\{
        \begin{aligned}
            &\frac{f(x)-f(x_0)}{x-x_0}, && \text{if } x\neq x_0;\\
            &f^\prime(x_0), && \text{if } x = x_0.
        \end{aligned}\right.
    $$
    where it is the definition $g(x_0) = f^\prime(x_0)$ that avoids the trifling discussion of the third condition in Th \{, ID: 2.1.1\}.
\end{Th}

\begin{Th}{Th3.3.3 (derivative of inverse function)}
    Suppose real functions $f$ is continuous and strictly monotonic on an interval $I = (a,b)$ (where $a, b\in\mathbb{R}_\infty$). If $f$ is derivable at some $x_0\in I$ and $f^\prime(x_0)\neq 0$, then the inverse function $f^{-1}$ is derivable at $y_0 = f(x_0)$ and
    $$ (f^{-1})^\prime(y_0) = \frac{1}{f^\prime(x_0)}. $$
    \tcblower
    \textit{Pf}: Trivial.
\end{Th}

\begin{Df}{Df3.4 (derivative function)}
    Suppose $f$ is a real function derivable at every point of a set $I\subseteq \mathbb{R}$. Then the function $x\mapsto f^\prime(x)$ defined on $I$ is called the \textbf{derivative function} of $f$ on $I$.
\end{Df}

\begin{Rmk}{}
    \textcolor{Df}{If $f$ is derivable at every point of $\text{dom}(f)$, then we called the derivative function of $f$ on $\text{dom}(f)$ the \textbf{derivative function} of $f$.}
\end{Rmk}

\begin{Df}{Df3.4.1 (higher derivatives)}
    Suppose $f$ is a real function and $x_0\in\mathbb{R}$. Define the $n$-th ($n\in\mathbb{N}$) derivative of $f$ at $x_0$, denoted by $f^{(n)}(x_0)$, inductively as follows:
    \begin{compactenum}
        \item $f^{(0)}(x_0) = f(x_0)$;
        \item $f^{(n)}(x_0) = [f^{(n-1)}]^\prime(x_0)$ for $n\geq 1$.
    \end{compactenum}
    We say that $f$ is $n$-times derivable at $x_0$ if $f^{(n)}(x_0)$ exists, and we call the map $x\mapsto f^{(n)}(x)$ defined on the set of all points where $f$ is $n$-times derivable the $n$-th derivative function of $f$, denoted by $f^{(n)}: \text{dom}(f^{(n)})\rightarrow \mathbb{R}$.
\end{Df}

\begin{Rmk}{}
    \textcolor{Df}{Suppose $f$ is a real function. Then $f$ is said to be $n$-times derivable on $I = (a,b)$ (where $a, b\in\mathbb{R}_\infty$) if $f$ is $n$-times derivable at every $x\in I$.} \textcolor{Th}{For a real function $f$, if $f$ is $n$-times derivable at $x_0$, then $f$ is $m$-times derivable at $x_0$ for every $m\leq n$.}
\end{Rmk}

\begin{Th}{Th3.4.2 (Leibniz's formula for computing higher derivatives)}
    Suppose real functions $f$ and $g$ are both $n$-times derivable on $I = (a,b)$ (where $a, b\in\mathbb{R}_\infty$). Then $fg$ is $n$-times derivable on $I$ and
    $$ (fg)^{(n)} = \sum_{k=0}^n \binom{n}{k} f^{(k)}g^{(n-k)}$$
    on $I$.
    \tcblower
    \textit{Pf}: Repeatedly apply the product rule of derivatives.
\end{Th}

\begin{Df}{Df3.5.-1 (interval-derivable)}
    Suppose $f$ is a real function and $I=[a,b]$ ($a,b\in\mathbb{R}$). We say that $f$ is \textbf{interval-derivable} on $I$ if $f$ is interval-continuous on $I$ and derivable on $(a,b)$.
\end{Df}

\begin{Rmk}{}
    The term ``interval-derivable'' is used as the premise of the following ``derivative intermediate value theorems''.
\end{Rmk}

\begin{Df}{Df3.5.0.-1 (extremum)}
    \begin{compactenum}
        \item Suppose $f$ is a real function and $x_0\in\mathbb{R}$. We say $x_0$ is a peak point of $f$ (resp. a valley point of $f$) and $f(x_0)$ is the corresponding peak (resp. valley) if there exists some $B_\delta(x_0)$ s.t. $f(x_0)\geq f(x)$ (resp. $f(x_0)\leq f(x)$) for every $x\in B_\delta(x_0)$.
        \item Suppose $f$ is a real function and $x_0\in\mathbb{R}$. We say $x_0$ is an extremal point of $f$ and $f(x_0)$ is the corresponding extremum if $x_0$ is a peak point or a valley point of $f$.
    \end{compactenum}
\end{Df}

\begin{Rmk}{}
    \textcolor{Th}{The definition of extremal points itself implies that $f$ must be well-defined near and at $x_0$,} as the statement ``$f(x_0)\leq f(x)$ for every $x\in B_\delta(x_0)$'' has already implied that $B_\delta(x_0)\subseteq\text{dom}(f)$. Also recall that \textcolor{Df}{Suppose $f$ is a function and $x_0$ is some element. Then $x_0$ is called a maximal point (resp. a minimal point) of $f$ and $f(x_0)$ is the corresponding maximum (resp. minimum) if $f(x_0)\geq f(x)$ (resp. $f(x_0)\leq f(x)$) for every $x\in \text{dom}(f)$.}
\end{Rmk}

\begin{Th}{Th3.5 (Fermat's theorem)}
    Suppose $f$ is a real function and $x_0\in\mathbb{R}$. If $x_0$ is an extremal point of $f$ and $f$ is derivable at $x_0$, then $f^\prime(x_0) = 0$.
    \tcblower
    \textit{Pf}: Let us suppose $x_0$ is a valley point of $f$. Then $f(x)\geq f(x_0)$ for every $x$ in some $B = B_\delta(x_0)$. Thus in $B$, 
    $$ \frac{f(x)-f(x_0)}{x-x_0} \left\{
        \begin{aligned}
            &\geq 0, && \text{if } x>x_0;\\
            &\leq 0, && \text{if } x<x_0.
        \end{aligned}\right.
    $$
    Hence the limit $f^\prime(x_0) = \lim\limits_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}$ must be both $\geq 0$ and $\leq 0$, namely, $f^\prime(x_0) = 0$.
\end{Th}

\begin{Rmk}{}
    \textcolor{Df}{Suppose $f$ is a real function and $x_0\in\mathbb{R}$. Then $x_0$ is called a stationary point of $f$ if $f^\prime(x_0) = 0$.}
\end{Rmk}

\begin{Th}{Th3.5.1 (Rolle's intermediate-value theorem)}
    Suppose $f$ is a real function that is interval-derivable on $[a,b]$ ($a,b\in\mathbb{R}$). If $f(a) = f(b)$, then there exists some $\xi \in (a,b)$ s.t. $f^\prime(\xi) = 0$.
    \tcblower
    \textit{Pf}: Since $f$ is interval-continuous on $[a,b]$, it has a maximal point $x^*$ and a minimal point $x_*$ on $[a,b]$. If $f(x^*) = f(x_*)$, then $f$ is constant on $[a,b]$ and $f^\prime(x) = 0$ for every $x\in (a,b)$; if $f(x^*) > f(x_*)$, then one of $x^*$ and $x_*$ must be in $(a,b)$ (since $f(a) = f(b)$), say $x^*\in (a,b)$. Then $x^*$ is a peak point of $f$ and thus $f^\prime(x^*) = 0$ by Fermat's theorem.
\end{Th}

\begin{Th}{Th3.5.2 (Lagrange's intermediate-value theorem)}
    Suppose $f$ is a real function that is interval-derivable on $[a,b]$ ($a,b\in\mathbb{R}$). Then there exists some $\xi\in (a,b)$ s.t.
    $$ f^\prime(\xi) = \frac{f(b)-f(a)}{b-a}. $$
    \tcblower
    \textit{Pf}: We can clearly see that this is an extension of Rolle's theorem, which inspires us to reduce the problem to the case of Rolle's theorem. Just seek a function $g$ from $f$ s.t. $g(a) = g(b)$, which is just to turn the rod $(a, f(a)), (b, f(b))$ into a horizontal one. Thus consider
    $$ g(x) = f(x) - \frac{f(b)-f(a)}{b-a}(x-a), $$
    which is as required. 
\end{Th}

\begin{Th}{Th3.5.3 (Cauchy's intermediate-value theorem)}
    Suppose $f$ and $g$ are real functions that are interval-derivable on $[a,b]$ ($a,b\in\mathbb{R}$) and $g^\prime(x)\neq 0$ for every $x\in (a,b)$. Then there exists some $\xi\in (a,b)$ s.t.
    $$ \frac{f^\prime(\xi)}{g^\prime(\xi)} = \frac{f(b)-f(a)}{g(b)-g(a)}. $$
    \tcblower
    \textit{Pf}: If proving with the Lagrange's theorem, then we can not force the point $\xi$ to be identical for $f$ and $g$. Thus still prove it with the Rolle's theorem. Write the expression as
    $$ f^\prime(\xi)\Big[g(b)-g(a)\Big] - g^\prime(\xi)\Big[f(b)-f(a)\Big] = 0. $$
    Hence we construct the function $h$ as
    $$ h(x) = f(x)\Big[g(b)-g(a)\Big] - g(x)\Big[f(b)-f(a)\Big], $$
    which can be verified to satisfy $h(a) = h(b)$, done.
\end{Th}

\begin{Th}{Th3.5.4 (Darboux's theorem)}
    Suppose $f$ is a real function that is derivable on $[a, b]$ ($a, b\in\mathbb{R}$). Then:
    \begin{compactenum}
        \item $f^\prime$ on $[a,b]$ can achieve all values between $f^\prime(a)$ and $f^\prime(b)$ (that is, say $f^\prime(a) \leq f^\prime(b)$, for every $\gamma$ s.t. $f^\prime(a)\leq\gamma\leq f^\prime(b)$, there exists some $\xi\in [a,b]$ s.t. $f^\prime(\xi) = \gamma$);
        \item On $(a,b)$, $f^\prime$ has no type-1 discontinuous point;
        \item If $\lim\limits_{x\to a^+} f^\prime(x)$ (resp. $\lim\limits_{x\to b^-} f^\prime(x)$) exists, then $f^\prime$ is right-continuous at $a$ (resp. left-continuous at $b$).
    \end{compactenum}
    \tcblower
    \textit{Pf}:
    \begin{compactenum}
        \item To show the derivative, namely, the slope of tangent lines traverse $[f^\prime(a), f^\prime(b)]$, we can show the slope of secant lines traverse and then apply the Lagrange's theorem. Hence we first construct the secant function, and show that it traverses. To show it traverses, just recall that a interval-continuous function traverses its range. Hence the secant function:
        $$ d_a(x) = \frac{f(x)-f(a)}{x-a}, $$
        which, at least, traverses $[d_a(a), d_a(b)]$. But $d_a(a)$ is not defined, hence we supplement it with:
        $$ d_a(x) = \left\{
            \begin{aligned}
                &\frac{f(x)-f(a)}{x-a}, && \text{if } x\neq a;\\
                &f^\prime(a), && \text{if } x = a.
            \end{aligned}\right.
        $$
        where $d_a(x)$ traverses $[d_a(a), d_a(b)] = \Big[f^\prime(a), \frac{f(b)-f(a)}{b-a}\Big]$.
        How about the left side $[\frac{f(b)-f(a)}{b-a}, f^\prime(b)]$? Just construct the secant function $d_b(x)$ in the same way:
        $$ d_b(x) = \left\{ 
            \begin{aligned}
                &\frac{f(b)-f(x)}{b-x}, && \text{if } x\neq b;\\
                &f^\prime(b), && \text{if } x = b.
            \end{aligned}\right.
        $$
        \item If $f^\prime$ has a type-1 discontinuous point $x_0\in (a,b)$, then $\lim\limits_{x\to x_0^+} f^\prime(x)$ and $\lim\limits_{x\to x_0^-} f^\prime(x)$ both exist but one of them is not equal to $f^\prime(x_0)$, say $\lim\limits_{x\to x_0^-} f^\prime(x) \neq f^\prime(x_0)$. Then from the locality of limit, we can find some small $[x_0-\delta, x_0)$ where $f^\prime(x)$ can not achieve some values between $f^\prime(x_0)$ and $\lim\limits_{x\to x_0^-}$, which contradicts the first part of this theorem (as the first part we just proved also holds on $[x_0-\delta, x_0]$).
        \item The same thought as the second part.  
    \end{compactenum}
\end{Th}

\begin{Th}{Th3.6.1 (monotonicity and derivatives)}
    Suppose $f$ is a real function that is interval-derivable on $[a,b]$ ($a,b\in\mathbb{R}$). Then $f$ is increasing (resp. decreasing) on $[a,b]$ iff $f^\prime(x)\geq 0$ (resp. $f^\prime(x)\leq 0$) for every $x\in (a,b)$.
    \tcblower
    \textit{Pf}: Prove the increasing case. ``if'' can be proved by the Lagrange's intermediate-value theorem. For ``only if'', suppose $f^\prime(x_0) < 0$ for some $x_0\in (a,b)$. Then by the locality of limit, there exists some $B_\delta(x_0)$ where $\frac{f(x)-f(x_0)}{x-x_0} < 0$, which contradicts the increasing property of $f$.
\end{Th}

\begin{Th}{Th3.6.1.1 (strict monotonicity and derivatives)}
    Suppose $f$ is a real function that is interval-derivable on $[a,b]$ ($a,b\in\mathbb{R}$). Then $f$ is strictly increasing (resp. strictly decreasing) on $[a,b]$ iff:
    \begin{compactenum}
        \item $f^\prime(x)\geq 0$ (resp. $f^\prime(x)\leq 0$) for every $x\in (a,b)$ and 
        \item for every open interval $I=(c,d)\subseteq (a,b)$, there exists some $x_0\in I$ s.t. $f^\prime(x_0) > 0$ (resp. $f^\prime(x_0) < 0$).
    \end{compactenum}
    \tcblower
    \textit{Pf}: The proof is trivial, and only take a look at the derivation of this theorem. Known that the not-strict increasing is equal to the non-negative derivative, we seek the condition for the strict increasing. Clearly if all the derivatives are positive then $f$ is strictly increasing. But this is too strong, as we force too many points to have positive derivatives. How many points should have positive derivatives to guarantee the strict increasing, i.e. $(f(x_2)-f(x_1)) / (x_2-x_1) > 0$ for every $x_1, x_2\in (a,b)$? Right here is that we need to force the presence of positive derivatives in every open interval $(x_1, x_2)$ in $I$.
\end{Th}

\begin{Df}{Df3.6.2.-1 (strict extremal and maximal/minimal)}
    \begin{compactenum}
        \item Suppose $f$ is a function valued in $\mathbb{R}$ and $x_0\in\text{dom}(f)$. Then $x_0$ is called a strict maximal point (resp. a strict minimal point) of $f$ and $f(x_0)$ is called the strict maximum (resp. strict minimum) if $f(x_0) > f(x)$ (resp. $f(x_0) < f(x)$) for every $x\in\text{dom}(f)\setminus\{x_0\}$.
        \item Suppose $f$ is a real function and $x_0\in\mathbb{R}$. Then $x_0$ is called a strict peak point of $f$ (resp. a strict valley point of $f$) and $f(x_0)$ is called the strict peak (resp. strict valley) if there exists some $B_\delta(x_0)$ s.t. $f(x_0) > f(x)$ (resp. $f(x_0) < f(x)$) for every $x\in B_\delta(x_0)\setminus\{x_0\}$.
        \item Strict peak points and strict valley points are called strict extremal points.
    \end{compactenum}
\end{Df}

\begin{Th}{Th3.6.2.1 (strict extremum and derivative)}
    Suppose $f$ is a real function that is interval-continuous on $[a,b]$ ($a,b\in\mathbb{R}$) and $x_0\in (a,b)$. If there exists some $\check{B}_\delta(x_0)$ s.t. 
    \begin{compactenum}
        \item $f^\prime(x) > 0$ (resp. $f^\prime(x) < 0$) for every $x\in (x_0-\delta, x_0)$ and 
        \item $f^\prime(x) < 0$ (resp. $f^\prime(x) > 0$) for every $x\in (x_0, x_0+\delta)$, then $x_0$ is a strict peak point (resp. a strict valley point) of $f$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: An extremal point, say a peak point $x_0$ often appears when $f(x)$ is increasing on some left side and decreasing on some right side of $x_0$. Trivial.
\end{Th}

\begin{Rmk}{}
    This theorem does not require the existence of $f^\prime(x_0)$, which can be appiled to such a function $f(x) = |x|$ at $x_0 = 0$.
\end{Rmk}

\begin{Th}{Th3.6.2.2 (Th3.6.2.1 given the 2nd derivative exists)}
    Suppose $f$ is a real function that is interval-continuous on $[a,b]$ ($a,b\in\mathbb{R}$) and $x_0\in (a,b)$. If $f^\prime (x_0) = 0$ and $f^{\prime\prime}(x_0)$ exists, then $x_0$ is a strict peak (resp. valley) point of $f$ if $f^{\prime\prime}(x_0) < 0$ (resp. $f^{\prime\prime}(x_0) > 0$).
    \tcblower
    \textit{Pf}: Trivial.
\end{Th}

\begin{Rmk}{}
    Given a function $f$ with nice properties, we can develop a procedure to find its maximum and minimum.

    Suppose $f$ is a real function that is interval-derivable on $[a,b]$ ($a,b\in\mathbb{R}$). Then $f$ has a maximum and a minimum on $[a,b]$ (since $f$ is interval-continuous on $[a,b]$). If a maximal point $x_0$ is in $(a,b)$, then $x_0$ is a peak point of $f$ and thus it must have $f^\prime(x_0) = 0$. If there are finitely many stationary points of $f$ in $(a,b)$, then we can search the maximal points within this range:
    $$ \max_{x\in [a,b]} f(x) = \max\{f(a), f(s_1), \cdots, f(s_n), f(b)\}, $$
    where $s_1, \cdots, s_n$ are the stationary points of $f$ in $(a,b)$. Likewise, we can find the minimal points:
    $$ \min_{x\in [a,b]} f(x) = \min\{f(a), f(s_1), \cdots, f(s_n), f(b)\}. $$

    If there are some non-derivable points of $f$ in $(a,b)$, then we have to take care of them. 
\end{Rmk}

\begin{Df}{Df3.6.3.-2 (convex set and convex function)}
    \begin{compactenum}
        \item Suppose $S$ is a set with addition and scalar multiplication (over $\mathbb{R}$) defined on it. Then for every $x, y\in S$ and $\lambda\in [0,1]$, the element $\lambda x + (1-\lambda)y$ of $S$ is called a convex combination of $x$ and $y$. 
        \item Suppose $S$ is a set with addition and scalar multiplication (over $\mathbb{R}$) defined on it. Suppose $A\subseteq S$. Then $A$ is called a convex set (or, $A$ is said to be convex) if for every $x, y\in A$, any convex combination of $x$ and $y$ is also in $A$.
        \item Suppose $f$ is a real function and $\varnothing\neq A\subseteq\text{dom}(f)$ where $A$ is convex. Then the restriction of $f$ on $A$ is called a convex function (resp. strict convex function) or $f$ is said to be convex on $A$ (resp. strict convex on $A$) if: \\
        $\forall x_1, x_2\in \text{dom}(f)$, $\forall \lambda_1, \lambda_2\in (0,1) \text{ s.t. } \lambda_1+\lambda_2=1$, we have $f(\lambda_1 x_1 + \lambda_2 x_2) \leq \lambda_1 f(x_1) + \lambda_2 f(x_2)$ (resp. $f(\lambda_1 x_1 + \lambda_2 x_2) < \lambda_1 f(x_1) + \lambda_2 f(x_2)$).
    \end{compactenum}
\end{Df}

\begin{Rmk}{}
    \textcolor{Df}{If the restriction of $f$ on $\text{dom}(f)$ is a convex function, then we say $f$ is a convex function.}\\
    We can see the natural geometric meaning of convex combination $\lambda x+(1-\lambda)y$: some point on the line segment connecting $x$ and $y$. Hence in $\mathbb{R}$ (we now just discuss the convex set in $\mathbb{R}$, although we will define the more general convex sets in the future), an interval is a convex set. But is the converse true? Yes.
\end{Rmk}

\begin{Th}{Th3.6.3.-1 (in $\mathbb{R}$, convex set is interval)}
    Suppose $A\subseteq \mathbb{R}$. Then $A$ is a convex set iff
    \begin{compactenum}
        \item $A=\varnothing$ or
        \item $A=\{x\}$ for some $x\in\mathbb{R}$ or
        \item $A$ is an interval.
    \end{compactenum}
    \tcblower
    \textit{Pf}: We can see that the empty set, a singleton set, and an interval are all convex sets. Conversely, suppose $A$ is a convex set, then we can further derive what $A$ is like. Consider the supremum and infimum in $\mathbb{R}_\infty$ of $A$, then $-\infty\leq\inf A \leq\sup A\leq\infty$, so that we can easily verify that $A$ is a singleton set or $A$ traverses the interval $(\inf A, \sup A)$.
\end{Th}

\begin{Th}{Th3.6.3 (Jensen's (詹森) inequality)}
    Suppose $f$ is a convex function. Then for every $n\geq 2$ and any $x_1, \cdots, x_n\in\text{dom}(f)$, 
    $$ f\left(\sum_{i}^{n}\lambda_i x_i\right) \leq \sum_{i}^{n}\lambda_i f(x_i) $$
    for any $\lambda_1, \cdots, \lambda_n\in (0,1)$ s.t. $\sum_{i}^{n}\lambda_i = 1$.
    \tcblower
    \textit{Pf}: The definition of convex function is just the Jensen's inequality for $n=2$. Now let us think about the induction, say $n=3$. Since $\lambda_1+\lambda_2+\lambda_3 = 1 = \lambda_3 + (\lambda_1+\lambda_2)$, we come to the idea:
    $$ 
    \begin{aligned}
        f\left(\lambda_3 x_3 + \lambda_1 x_1 + \lambda_2 x_2\right) &= f\left[\lambda_3 x_3 + (\lambda_1 + \lambda_2)\left(\frac{\lambda_1}{\lambda_1+\lambda_2}x_1 + \frac{\lambda_2}{\lambda_1+\lambda_2}x_2\right)\right] \\
        &\leq \lambda_3 f(x_3) + (\lambda_1+\lambda_2)f\left(\frac{\lambda_1}{\lambda_1+\lambda_2}x_1 + \frac{\lambda_2}{\lambda_1+\lambda_2}x_2\right) \\
        &\leq \lambda_3 f(x_3) + (\lambda_1+\lambda_2)\left(\frac{\lambda_1}{\lambda_1+\lambda_2}f(x_1) + \frac{\lambda_2}{\lambda_1+\lambda_2}f(x_2)\right) = \sum_{i=1}^{3}\lambda_i f(x_i).
    \end{aligned} 
    $$
\end{Th}

\begin{Rmk}{}
    For the strict convex function, we probably need the strict inequality. To make the strict inequality hold, we just need to make one of the two ``$\leq$'' in the proof above strict. For $n=2$, the strict inequality requires that $x_1\neq x_2$, which is the second ``$\leq$''. If $x_1=x_2=x$ and thus the second ``$\leq$'' is ``='', then the first ``$\leq$'' must be strict so that it requires
    $$ x_3\neq \frac{\lambda_1}{\lambda_1+\lambda_2}x_1 + \frac{\lambda_2}{\lambda_1+\lambda_2}x_2 = x,$$
    which indicates that $x_1, x_2, x_3$ are not all equal. Hence we have the strict Jensen's inequality. \textcolor{Th}{Suppose $f$ is a strict convex function. Then for every $n\geq2$ and any $x_1, \cdots, x_n\in\text{dom}(f)$ s.t. $x_1, \cdots, x_n$ are not all equal,
    $$ f\left(\sum_{i}^{n}\lambda_i x_i\right) < \sum_{i}^{n}\lambda_i f(x_i) $$
    for any $\lambda_1, \cdots, \lambda_n\in (0,1)$ s.t. $\sum_{i}^{n}\lambda_i = 1$.}
    Now it comes to the question: how to judge whether a function is convex?
\end{Rmk}

\begin{Th}{Th3.6.3.1 (convex means increasing slope of secant lines)}
    Suppose $f$ is a real function defined on an interval $I$. Then $f$ is convex on $I$ (resp. strict convex on $I$) iff for every $(x_1, x_2)\subseteq I$ and $x\in (x_1, x_2)$, 
    $$ \frac{f(x)-f(x_1)}{x-x_1} \leq \frac{f(x_2)-f(x_1)}{x_2-x_1} \leq \frac{f(x_2)-f(x)}{x_2-x}. $$
    (resp. $<$ for all the ``$\leq$'''s above)
    \tcblower
    \textit{Pf}: The derivation of this theorem is trivial. Here is a compact proof:
    $$
    \begin{aligned}
        &\frac{f(x)-f(x_1)}{x-x_1} \leq \frac{f(x_2)-f(x_1)}{x_2-x_1} \\
        \Leftrightarrow &\frac{f(\lambda_1 x_1 + \lambda_2 x_2) - f(x_1)}{\lambda_1 x_1 + \lambda_2 x_2-x_1} \leq \frac{f(x_2)-f(x_1)}{x_2-x_1} \\
        \Leftrightarrow &\frac{f(\lambda_1 x_1 + \lambda_2 x_2) - f(x_1)}{\lambda_2 (x_2-x_1)} \leq \frac{f(x_2)-f(x_1)}{x_2-x_1} \\
        \Leftrightarrow &f(\lambda_1 x_1 + \lambda_2 x_2) \leq \lambda_1 f(x_1) + \lambda_2 f(x_2).
    \end{aligned}
    $$
\end{Th}

\begin{Rmk}{}
    The equality above can be simplified as 
    $$ \frac{f(x)-f(x_1)}{x-x_1} \leq \frac{f(x_2)-f(x)}{x_2-x}, $$
    since if these two slopes have this, then the slope of the secant line connecting $(x_1, f(x_1))$, $(x_2, f(x_2))$ is between these two slopes (recall the ``sugar water'' inequality). 
\end{Rmk}

\begin{Th}{Th3.6.3.2 (convexity and derivatives)}
    Suppose $f$ is a real function that is interval-derivable on $[a,b]$ ($a,b\in\mathbb{R}$). Then $f$ is convex on $[a,b]$ (resp. strict convex on $[a,b]$) iff $f^\prime$ is increasing on $(a,b)$ (resp. strictly increasing on $(a,b)$).
    \tcblower
    \textit{Pf}: Prove it with the Th \{, ID: 3.6.3.1\}, and then the ``if'' part immediately follows from the Lagrange's intermediate-value theorem. 

    For the ``only if'' part, suppose $f$ is convex on $[a,b]$. Then given $a<x_1<x_2<b$, we prove that $f^\prime(x_1)\leq f^\prime(x_2)$. To infer something about derivatives from the slope of secant lines (as what we know by the Th \{, ID: 3.6.3.1\} is the increasing property of the slope of secant lines), we cannot use those intermediate-value theorems as these infer something about the secant lines from the derivatives. Instead, we can use the ``secant functions'' $d_{x_1}(\cdot)$, $d_{x_2}(\cdot)$ defined in the proof of the Darboux's theorem. Since 
    $$ d_{x_1}(x) = \left\{
        \begin{aligned}
            &\frac{f(x)-f(x_1)}{x-x_1}, && \text{if } x\neq x_1;\\
            &f^\prime(x_1), && \text{if } x = x_1,
        \end{aligned}\right.
    $$
    we can see that $d_{x_1}(x)$ is increasing on $(x_1, x_2]$ (Th \{, ID: 3.6.3.1\}), and thus so is on $[x_1, x_2]$ (since $d_{x_1}$ is right-continuous at $x_1$). Hence $f^\prime(x_1) = d_{x_1}(x_1) \leq d_{x_1}(x_2) = \frac{f(x_2)-f(x_1)}{x_2-x_1}$. Likewise, we can show that $\frac{f(x_2)-f(x_1)}{x_2-x_1} = d_{x_2}(x_1) \leq d_{x_2}(x_2) = f^\prime(x_2)$ since $d_{x_2}$ is also increasing on $[x_1, x_2]$, so that $f^\prime(x_1)\leq f^\prime(x_2)$.
    
    Same thought for the case of strict convexity.
\end{Th}

\begin{Th}{Th3.6.3.3 (convexity and the 2nd-order derivative)}
    Suppose $f$ is a real function that is interval-derivable on $[a,b]$ ($a,b\in\mathbb{R}$), and $f^{\prime\prime}$ exists on $(a,b)$. Then:
    \begin{compactenum}
        \item $f$ is convex on $[a,b]$ iff $f^{\prime\prime}(x)\geq 0$ for every $x\in (a,b)$;
        \item $f$ is strict convex on $[a,b]$ iff (1) $f^{\prime\prime}(x)\geq 0$ for every $x\in (a,b)$ and (2) for any $(c,d)\subseteq (a,b)$ there exists some $x_0\in (c,d)$ s.t. $f^{\prime\prime}(x_0) > 0$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: This theorem can be drawn combining the Th \{, ID: 3.6.1.1\} and the Th \{, ID: 3.6.3.2\} (perhaps supplement the definition of $f^\prime$ at $a$ and $b$ with the corresponding limits to meet the conditions that $f^\prime$ is interval-derivable on $[a,b]$).
\end{Th}

\begin{Th}{Th3.7.1 (L'Hospital's (洛必达) rule for $\frac{0}{0}$)}
    Within the range of this theorem (including theorem text, proof and corresponding remarks), we adopt the real circle's perspective.\\
    Suppose $f$ and $g$ are real functions, and $x_0, y_0$ are arbitrarily chosen from $t_0, t_0^+, t_0^-, \infty, \infty^+, \infty^-$ ($t_0\in\mathbb{R}$). If:
    \begin{compactenum}
        \item $f$ and $g$ are derivable on some $\check{U}(x_0)$;
        \item There is some $\check{U}(x_0)$ for $x$ where $g(x)\neq 0$;
        \item $\lim\limits_{x\to x_0} f(x) = \lim\limits_{x\to x_0} g(x) = 0$;
        \item $\lim\limits_{x\to x_0} \frac{f^\prime(x)}{g^\prime(x)} = y_0$;
    \end{compactenum}
    then $\lim\limits_{x\to x_0} \frac{f(x)}{g(x)} = \lim\limits_{x\to x_0} \frac{f^\prime(x)}{g^\prime(x)} = y_0$.
    \tcblower
    \textit{Pf}: First we prove the case where $x_0 = t_0$. Supplement the definition of $f$ and $g$ at $x_0$ with the corresponding limits
    $$ f(x_0) = \lim\limits_{x\to x_0} f(x) = 0, \quad g(x_0) = \lim\limits_{x\to x_0} g(x) = 0. $$
    Then:
    $$ \lim\limits_{x\to x_0} \frac{f(x)}{g(x)} = \lim\limits_{x\to x_0} \frac{f(x)-f(x_0)}{g(x)-g(x_0)} = \lim\limits_{x\to x_0} \frac{f^\prime(\xi(x))}{g^\prime(\xi(x))} = \lim\limits_{\xi\to x_0} \frac{f^\prime(\xi)}{g^\prime(\xi)} = y_0. $$
    From the left to the right, the second equality is the Cauchy's intermediate-value theorem, where we apply it for every interval with end points $x$ and $x_0$ and find a $\xi(x)$ for every $x$. Hence $\xi(\cdot)$ is a function of $x$ and $\lim\limits_{x\to x_0} \xi(x) = x_0$. And the third equality is the limit of a function composition. \\
    As for $x_0=t_0^+$ and $x_0=t_0^-$, we can prove them in the same way.\\
    Now we prove the case where $x_0 = \infty$. Let $x = 1/t$, then we apply the L'Hospital's rule proved above to $\lim\limits_{t\to 0} \frac{f(1/t)}{g(1/t)}$.\\
    As for $x_0 = \infty^+$ and $x_0 = \infty^-$, we can prove them in the same way. Then we complete the proof.
\end{Th}

\begin{Th}{Th3.7.2 (L'Hospital's rule for $\frac{\infty}{\infty}$)}
    Suppose $f$ and $g$ are real functions. Let $x_0$ be a one-sided limit target, and $y_0$ be an option from $t_0^+, t_0^-, t_0, \infty, -\infty$. If:
    \begin{compactenum}
        \item $f$ and $g$ are derivable on some $\check{U}(x_0)$;
        \item $\lim\limits_{x\to x_0} g(x) = \pm\infty$;
        \item $\lim\limits_{x\to x_0} \frac{f^\prime(x)}{g^\prime(x)} = y_0$;
    \end{compactenum}
    then $\lim\limits_{x\to x_0} \frac{f(x)}{g(x)} = \lim\limits_{x\to x_0} \frac{f^\prime(x)}{g^\prime(x)} = y_0$.
    \tcblower
    \textit{Pf}: See the next page.
\end{Th}

\begin{Th}{Th3.7.2 (L'Hospital's rule for $\frac{\infty}{\infty}$) —  continued}
    \textit{Pf}: We only prove the case where $x_0 = t_0^+$ and $y_0 = t_0$, and the other cases can be proved with the same idea.\\
    Let $x_0\in\mathbb{R}$ and the limit target is $x_0^+$. Since $\lim\limits_{x\to x_0} \frac{f^\prime(x)}{g^\prime(x)} = y_0$, for every $\epsilon>0$, there exists some $\delta>0$ s.t. $y_0-\varepsilon < \frac{f^\prime(x)}{g^\prime(x)} < y_0+\varepsilon$ for every $x\in (x_0, x_0+\delta)$. Then take an arbitrary interval $(x,c)\subseteq (x_0, x_0+\delta)$, and apply the Cauchy's intermediate-value theorem to find some $\xi\in (x,c)$ s.t. 
    $$ \frac{f(x)-f(c)}{g(x)-g(c)} = \frac{f^\prime(\xi)}{g^\prime(\xi)}\in (y_0-\varepsilon, y_0+\varepsilon). $$
    Since also 
    $$ \frac{f(x)-f(c)}{g(x)-g(c)} = \left[\frac{f(x)}{g(x)}-\frac{f(c)}{g(x)}\right]\left[1-\frac{g(c)}{g(x)}\right]^{-1}, $$
    Take the limit superior and limit inferior of the both sides of the equation above, we have
    $$ 
    \begin{aligned}
        \limsup\limits_{x\to x_0^+} \frac{f(x)}{g(x)} &= \limsup\limits_{x\to x_0} \Biggl\{\left[\frac{f(x)}{g(x)}-\frac{f(c)}{g(x)}\right]\left[1-\frac{g(c)}{g(x)}\right]^{-1}\Biggr\} \\ 
        &= \limsup\limits_{x\to x_0^+} \frac{f^\prime(\xi)}{g^\prime(\xi)}\in [y_0-\varepsilon, y_0+\varepsilon], 
    \end{aligned}
    $$
    and we similarly get $\liminf\limits_{x\to x_0^+} \frac{f(x)}{g(x)}\in [y_0-\varepsilon, y_0+\varepsilon]$. Then let $\varepsilon\rightarrow 0$, we get 
    $$ \liminf\limits_{x\to x_0^+} \frac{f(x)}{g(x)} = \limsup\limits_{x\to x_0^+} \frac{f(x)}{g(x)} = y_0 = \lim\limits_{x\to x_0^+} \frac{f(x)}{g(x)}. $$ 
\end{Th}

\begin{Rmk}{}
    Let us talk about the idea of this proof. To link $\lim\frac{f(x)}{g(x)}$ to $\lim\frac{f^\prime(x)}{g^\prime(x)}$, the Cauchy's intermediate-value theorem is still the only key so far. But we cannot directly supplement the definition of $f$ and $g$ at $x_0$ to write the equality
    $$ \frac{f(x)}{g(x)} = \frac{f(x)-f(x_0)}{g(x)-g(x_0)} = \frac{f(x)-f(x_0)}{g(x)-(\pm\infty)}. $$
    Hence we can only apply the Cauchy's intermediate-value theorem to one sub-interval $(x,c)$ of $(x_0, x_0+\delta)$, and then study the limit process of $\frac{f(x)-f(c)}{g(x)-g(c)}$ with $x\rightarrow x_0^+$, hoping that 
    $$\frac{f(x)-f(c)}{g(x)-g(c)}\approx \frac{f(x)}{g(x)}.$$ 
    Then why does the approximation hold? The answer is that $g(x)\rightarrow\pm\infty$, which covers the influence of $f(c)$ and $g(c)$ in the limit process. Remember when we solve the limit, say, 
    $$ \lim\limits_{x\to\infty} \frac{2x-123}{3x-456}, $$
    we usually divide the numerator and the denominator by $x$ to make them finite in the limit process. This is the same idea here where we just divide the numerator and the denominator by $g(x)$. Afterwards, we take the limit superior and limit inferior as we cannot preclaim the existence of the limit.\\
    Here is some extension of this theorem. \textcolor{Th}{You might want to let $x_0$ be two-sided. Then what you need is just to add the extra condition:
    $$ \forall\check{U}(x_0^-)\forall\check{U}(x_0^+)\;\exists x_1\in\check{U}(x_0^-)\exists x_2\in\check{U}(x_0^+) \text{ s.t. } g^\prime(x_1)\neq 0\neq g^\prime(x_2). $$
    After we add this condition, the two-sided L'Hospital's rule can be reduced to the one-sided cases. \\
    We do not consider $y_0$ to be $\pm\infty$ now.}
\end{Rmk}
\end{document}