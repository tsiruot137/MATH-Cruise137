\documentclass{article}

    \usepackage{xcolor}
    \definecolor{pf}{rgb}{0.4,0.6,0.4}
    \usepackage[top=1in,bottom=1in, left=0.8in, right=0.8in]{geometry}
    \usepackage{setspace}
    \setstretch{1.2} 
    \setlength{\parindent}{0em}

    \usepackage{paralist}
    \usepackage{cancel}

    % \usepackage{ctex}
    \usepackage{amssymb}
    \usepackage{amsmath}

    \usepackage{tcolorbox}
    \definecolor{Df}{RGB}{0, 184, 148}
    \definecolor{Th}{RGB}{9, 132, 227}
    \definecolor{Rmk}{RGB}{215, 215, 219}
    \newtcolorbox{Df}[2][]{colbacktitle=Df, colback=white, title={\large\color{white}#2},fonttitle=\bfseries,#1}
    \newtcolorbox{Th}[2][]{colbacktitle=Th, colback=white, title={\large\color{white}#2},fonttitle=\bfseries,#1}
    \newtcolorbox{Rmk}[2][]{colbacktitle=Rmk, colback=white, title={\large\color{black}{Remarks}},fonttitle=\bfseries,#1}

    \title{\LARGE \textbf{Inner Product Space}}
    \author{\large Jiawei Hu}

\begin{document}
\maketitle

This is the 7th chapter of advanced algebra, which is about \textbf{Inner Product Space}\\
Here it is necessary to claim a ``definition (Df) -> theorem (Th)'' working cycle, which acts as the writing style throughout this whole course. This working cycle is shown below:

\noindent\rule{\textwidth}{2pt}
\begin{Df}{Some Definition}
    The text of this definition.
\end{Df}

\begin{Rmk}{}
    The text of the remarks about the definition just proposed (possibly including what it means and what it is for).\\
    \textcolor{Df}{Some remarks with some incidental definitions.}\\
    \textcolor{Th}{Some remarks with some incidental theorems.}
\end{Rmk}

\begin{Th}{Some Theorem}
    The text of this theorem.
    \tcblower
    \textit{Pf}: The proof of this theorem (is possibly "todo" when the author cannot complete it yet).
\end{Th}

\begin{Rmk}{}
    The text of the remarks about the definition just proposed (possibly including what it means and what it is for).\\
    \textcolor{Df}{Some remarks with some incidental definitions.}\\
    \textcolor{Th}{Some remarks with some incidental theorems.}
\end{Rmk}
\noindent\rule{\textwidth}{2pt}
As for the text of both a definition or a theorem, a common fixed pattern of sentences is adopted, which is ``Suppose \dots (some pre-conditions or background information). Then \dots (the direct text for the definition or the theorem).''. Please identify this pattern later by yourself. 

By the way, we now reiterate some commonly-used notations and conventions:
\begin{compactenum}
    \item $\mathbb{C}$: the set of the complex numbers;
    \item $\mathbb{R}$: the set of the real numbers;
    \item $\mathbb{R}^+$: the set of the positive real numbers;
    \item $\mathbb{Q}$: the set of the rational numbers;
    \item $\mathbb{Z}$: the set of the integers;
    \item $\mathbb{N}$: the set of the natural numbers;
    \item $\mathbb{N^\ast}$ or $\mathbb{N}^+$: the set of the positive integers.
    \item $\sideset{^R}{}{\mathop{D}}$: the set of all functions from $D$ to $R$ (with domain $D$ and range in $R$).
    \item An agreement for the length of a list: if we write $a_1, \dots, a_n$, then we indicate that $n$ is finite and that $n\geq 1$; if we write $a_0, \dots, a_n$, then we indicate that $n$ is finite and that $n\geq 0$.
    \item $A\times B$: the Cartesian product of $A$ and $B$.
    \item $\mathbb{F}$: a number field.
    \item Continue to use the notations and concepts of functions (see the chapter 1 of course 0).
    \item The matrix product $KA$ is referred as ``$A$ left-multiplied by $K$'' or ``left-multiply $A$ by $K$''; $AK$ is similar.
\end{compactenum} 
Please check the notations and definitions by yourself from the previous chapters or courses. Then with everything prepared, here we go.

\begin{Df}{$\bullet$ Df7.1 (inner product)}
    Suppose $V$ is a vector space over $\mathbb{F}$. An \textbf{inner product} on $V$ is a function $\langle \cdot, \cdot \rangle: V\times V\to \mathbb{F}$ that satisfies the following properties:
    \begin{compactenum}
        \item (positivity) $\forall v\in V$, $\langle v, v\rangle$ is a non-negative real number;
        \item (definiteness) $\forall v\in V$: $\langle v, v\rangle = 0$ iff $v=0$;
        \item (additivity of the 1st slot) $\forall u, v, w\in V$, $\langle u+v, w\rangle = \langle u, w\rangle + \langle v, w\rangle$;
        \item (homogeneity of the 1st slot) $\forall u, v\in V$ and $\forall \lambda\in \mathbb{F}$, $\langle \lambda u, v\rangle = \lambda \langle u, v\rangle$;
        \item (conjugate symmetry) $\forall u, v\in V$, $\langle u, v\rangle = \overline{\langle v, u\rangle}$.
    \end{compactenum}
\end{Df}

\begin{Rmk}{}
    The concept of inner product is a generalization of the dot product in $\mathbb{F}^n$. We can then derive the following basic properties of the inner product $\langle \cdot, \cdot\rangle: V\times V\rightarrow \mathbb{F}$:
    \textcolor{Th}{
    \begin{compactenum}
        \item $\forall u$, the map $\langle \cdot, u\rangle$ is a linear functional on $V$;
        \item $\forall v\in V$, $\langle v, 0\rangle = \langle 0, v\rangle = 0$;
        \item (additivity of the 2nd slot) $\forall u, v, w\in V$, $\langle u, v+w\rangle = \langle u, v\rangle + \langle u, w\rangle$;
        \item (homogeneity of the 2nd slot) $\forall u, v\in V$ and $\forall \lambda\in \mathbb{F}$, $\langle u, \lambda v\rangle = \overline{\lambda}\langle u, v\rangle$.
    \end{compactenum}
    }
\end{Rmk}

\begin{Df}{$\bullet$ Df7.1.1 (inner product space)}
    An inner product space is a vector space equipped with an inner product.
\end{Df}

\begin{Rmk}{}
    In practice, the most commonly used inner product in $\mathbb{R}^n$ is the vector dot product (check the definition online by yourself).
\end{Rmk}

\begin{Df}{$\bullet$ Df7.2 (norm)}
    Suppose $V$ is a inner product space. The norm of a vector $v\in V$ is defined as $\|v\| = \sqrt{\langle v, v\rangle}$.
\end{Df}

\begin{Rmk}{}
    Norm is a generalization of the length of a vector in euclidean space. 
    \textcolor{Th}{Suppose $V$ is an inner product space. Then the basic properties of norm:
    \begin{compactenum}
        \item $\forall v\in V$, $\|v\|=0$ iff $v=0$;
        \item $\forall v\in V, \lambda\in\mathbb{F}$, $\|\lambda v\| = |\lambda|\cdot \|v\|$.
    \end{compactenum}}
\end{Rmk}

\begin{Df}{$\bullet$ Df7.3 (orthogonality)}
    Suppose $V$ is an inner product space and $u, v\in V$. We say $u$ and $v$ are orthogonal, denoted by $u\perp v$, if $\langle u, v\rangle = 0$.
\end{Df}

\begin{Rmk}{}
    The concept of orthogonality is a generalization of the concept of perpendicularity in euclidean space. We can see that \textcolor{Th}{in an inner product space, $0$ is orthogonal to all vectors, and $0$ is the only one in $V$ that is orthogonal to itself.} And it is important for us to extend this definition to a list of vectors: \textcolor{Df}{A list $\{v_1, \dots, v_n\}$ of vectors in an inner product space is said to be orthogonal if they are pairwise orthogonal.}
\end{Rmk}

\begin{Th}{$\circ$ Th7.3.1.1 (orthogonality and linear combinations)}
    Suppose $V$ is an inner space and $v$ is orthogonal to $u_1, \dots, u_m$ ($m=1,2,\dots$) respectively. Then $v$ is orthogonal to every linear combination of $u_1, \dots, u_m$
    \tcblower
    \textit{Pf}: Trivial. 
\end{Th}

\begin{Th}{$\circ$ Th7.3.1.2 (orthogonality implies linear independence)}
    In an inner space, a list of orthogonal vectors must be linearly independent.
    \tcblower
    \textit{Pf}: Trivial.
\end{Th}

\begin{Df}{$\bullet$ Df7.3.1 (parallelism)}
    Suppose $V$ is an inner product space and $u, v\in V$. We say that:
    \begin{compactenum}
        \item $u$ and $v$ are parallel if $\exists \lambda\in\mathbb{F}$ such that $u = \lambda v$ or $v = \lambda u$.
        \item $u$ and $v$ share the same orientation if $\exists \lambda\in\mathbb{R}^+\cup \{0\}$ s.t. $u = \lambda v$ or $v = \lambda u$.
        \item $u$ and $v$ are of the opposite orientation if $\exists \lambda\in\mathbb{R}^+\cup \{0\}$ s.t. $u = -\lambda v$ or $v = -\lambda u$. 
    \end{compactenum}
\end{Df}

\begin{Th}{$\bullet$ Th7.3.2 (Pythagorean theorem)}
    Suppose $V$ is an inner product space and $u, v\in V$. If $u$ and $v$ are orthogonal, then $\|u+v\|^2 = \|u\|^2 + \|v\|^2$.
    \tcblower
    \textit{Pf}: Obvious using the properties of the inner product.
\end{Th}

\begin{Rmk}{}
    We also have the ``parallelogram law'' in the inner product space: \textcolor{Th}{For any $u, v\in V$, $\|u+v\|^2 + \|u-v\|^2 = 2\|u\|^2 + 2\|v\|^2$.} This is a generalization of the fact: for a parallelogram, the sum of the squared lengths of the diagonals is equal to the sum of the squared lengths of the sides.
\end{Rmk}

\begin{Th}{$\bullet$ Th7.3.3 (orthogonal projection)}
    Suppose $V$ is an inner product space. Suppose also $u, v\in V$ and $u\neq 0$. \textcolor{Df}{Define the orthogonal projection of $v$ onto $u$ as the vector $\text{Proj}(v|u)$ such that $\text{Proj}(v|u)$ is parallel to $u$ and $v-\text{Proj}(v|u)$ is orthogonal to $u$.} Then we have:
    \begin{compactenum}
        \item $\text{Proj}(v|u)$ exists and is unique;
        \item $\text{Proj}(v|u) = \frac{\langle v, u\rangle}{\langle u, u\rangle}u$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: Trivial.
\end{Th}

\begin{Th}{$\bullet$ Th7.3.4 (Cauchy-Schwarz inequality)}
    Suppose $V$ is an inner product space. Then $\forall u, v\in V$: 
    $$|\langle u, v\rangle | \leq \|u\|\cdot \|v\|.$$
    And the equality holds iff $u$ and $v$ are parallel.
    \tcblower
    \textit{Pf}: Trivial if you notice that a vector is never shorter than its orthogonal projection.
\end{Th}

\begin{Th}{$\bullet$ Th7.3.4.1 (average inequality)}
    Suppose $a_1, \cdots, a_n\in\mathbb{R}^+$ ($n\geq 1$). Then:
    $$ H_n \leq G_n \leq A_n \leq Q_n,$$
    where 
    $$ H_n = \frac{n}{\frac{1}{a_1} + \cdots + \frac{1}{a_n}} \\
    G_n = \sqrt[n]{a_1\cdots a_n} \\
    A_n = \frac{a_1 + \cdots + a_n}{n} \\
    Q_n = \sqrt{\frac{a_1^2 + \cdots + a_n^2}{n}} $$
    are called the \textbf{harmonic mean}, \textbf{geometric mean}, \textbf{arithmetic mean}, and \textbf{quadratic mean} of $a_1, \cdots, a_n$ respectively. \\
    Here each inequality is an equality iff $a_1 = \cdots = a_n$.
    \tcblower
    \textit{Pf}: We first prove $G_n\leq A_n$, recording Cauchy's proof here. \\
    For $n=2$, we have proved that during our middle school. For $n=4$, we have:
    $$\begin{aligned}
        & \frac{a_1+a_2+a_3+a_4}{4} = \frac{\frac{a_1+a_2}{2} + \frac{a_3+a_4}{2}}{2} \\
        & \geq \frac{\sqrt{a_1a_2} + \sqrt{a_3a_4}}{2} \geq \sqrt{\sqrt{a_1a_2}\sqrt{a_3a_4}} = \sqrt[4]{a_1a_2a_3a_4}.
    \end{aligned}$$
    and so forth we can prove that $G_n\leq A_n$ holds for any $n = 2^k$. \\
    For general $n$, let $k$ be the smallest integer such that $2^k > n$, and let $n+r = 2^k$, $A = (a_1+\cdots+a_n)/n$. Then apply what we have just proved to the $2^k$ numbers 
    $$a_1, \cdots, a_n, \underbrace{A, \cdots, A}_{r}$$
    to complete the proof. \\
    Now we have $G_n\leq A_n$, with the equality iff $a_1 = \cdots = a_n$. Then for $H_n\leq G_n$, just replace $a_i$ in $G_n\leq A_n$ with $1/a_i$. For $A_n\leq Q_n$, just apply the Cauchy-Schwarz inequality (for the inner product space $\mathbb{R}^n$) to the vectors $(a_1, \cdots, a_n)$ and $(1, \cdots, 1)$.
\end{Th}

\begin{Th}{$\bullet$ Th7.3.5 (triangle inequality)}
    Suppose $V$ is an inner product space. Then $\forall u, v\in V$:
    $$\|u+v\|\leq \|u\| + \|v\|.$$
    And the equality holds iff $u$ and $v$ share the same orientation.
    \tcblower
    \textit{Pf}: Trivial.
\end{Th}

\begin{Rmk}{}
    This theorem is a generalization of the fact that ``the sum of the lengths of two sides of a triangle is greater than the length of the third side''.
\end{Rmk}

\begin{Df}{$\bullet$ Df7.4 (orthonormal)}
    Suppose $V$ is an inner product space and $v_1, \dots, v_n\in V$. We say that the list $\{v_1, \dots, v_n\}$ is orthonormal if it is orthogonal and $\|v_i\| = 1$ for all $i$.
\end{Df}

\begin{Rmk}{}
    In many cases, especially in many application scenarios of statistics, the assumption of orthogonal is very useful and elegent. \textcolor{Df}{An orthonormal basis is a basis consists of orthonormal vectors}, and we can verify that: \textcolor{Th}{if $e_1, \dots, e_n$ is an orthonormal basis of an inner space, then $\forall v = \sum_{i} a_i e_i\in V$, $\|v\| = \sum_{i}a_i^2$.} 
\end{Rmk}

\begin{Th}{$\bullet$ Th7.4.1 (coordinates under orthonormal basis)}
    Suppose $V$ is an finite-dimensional inner product space and $\{e_1, \dots, e_n\}$ is an orthonormal basis of $V$. Then for any $v\in V$:
    $$v = \sum_{i=1}^{n}\langle v,e_i\rangle e_i$$
    \tcblower
    \textit{Pf}: Trivial.
\end{Th}

\begin{Th}{$\bullet$ Th7.4.2 (Gram-Schmidt orthonormalization)}
    Suppose $V$ is an inner product space and $(v_1, \dots, v_n)$ is a linearly independent list of vectors in $V$. Then the list $(e_1, \dots, e_n)$ defined by the following recursive process (called the Gram-Schmidt orthonormalization process) is an orthonormal basis of $\text{span}(v_1, \dots, v_n)$:
    \begin{compactenum}
        \item $e_1 = \frac{v_1}{\|v_1\|}$;
        \item $e_k = \frac{v_k - \sum_{i=1}^{k-1}\langle v_k, e_i\rangle e_i}{\|v_k - \sum_{i=1}^{k-1}\langle v_k, e_i\rangle e_i\|}$ for $k=2, \dots, n$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: The idea of the process is just to removing the projection components of each basis vector to the previously construct subspace, using also the theorem \{, ID: 7.4.1\}.
\end{Th}

\begin{Rmk}{}
    \textcolor{Th}{If $v_1, \dots, v_n$ is already orthogonal, then the Gram-Schmidt process yields $\frac{v_1}{\|v_1\|}, \dots, \frac{v_n}{\|v_n\|}$; if $v_1, \dots, v_n$ is already orthonormal, then the process yields $v_1, \dots, v_n$ itself.} Since Gram-Schmidt is a recursive process, it can be cut off at any step while keeping its property (orthonormality and the same spanning).
    This theorem also indicates that \textcolor{Th}{every non-zero finite-dimensional inner product space has a orthonormal basis, and every orthogonal (resp. orthonormal) list of vectors in this space can be extended to an orthogonal (resp. orthonormal) basis.} For a given independent list of vectors, we can also first orthogonalize them, and then normalize them to find an orthonormal list of vectors.
\end{Rmk}

\begin{Th}{$\circ$ Th7.4.2.1 (upper-triangular matrix w.r.t. orthonormal basis)}
    Suppose $T\in\mathcal{L}(V)$ where $V$ is a non-zero finite-dimensional inner product space. If $\mathcal{M}(T)$ is upper-triangular w.r.t. some basis of $V$, then $\mathcal{M}(T)$ is upper-triangular w.r.t. some orthonormal basis of $V$.
    \tcblower
    \textit{Pf}: Just perform the Gram-Schmidt process on the known basis, and the resulted orthonormal basis is as desired.
\end{Th}

\begin{Th}{$\circ$ Th7.4.3 (Schur theorem)}
    Suppose $V$ is a non-zero finite-dimensional inner product space over $\mathbb{C}$ and $T\in\mathcal{L}(V)$. Then $T$ has an upper-triangular matrix w.r.t. some orthonormal basis.
    \tcblower
    \textit{Pf}: Trivial using Th\{, ID: 6.3.2\} and Th \{, ID: 7.4.2.1\}.
\end{Th}

\begin{Th}{$\bullet$ Th7.5 (Riesz representation theorem)}
    Suppose $V$ is a finite-dimensional inner product space. Then for any linear functional $\varphi\in V^\ast$, there exists a unique vector $u\in V$ such that $\varphi = \langle \cdot, u\rangle$.
    \tcblower
    \textit{Pf}: We have known that $\langle \cdot, u\rangle$ is a linear functional, which inspires us to consider the linear map $u\mapsto \langle \cdot, u\rangle$. Then our proof is just to show that this map is bijective. Trivial.
\end{Th}

\begin{Rmk}{}
    In Riesz representation, we might further be interested in what $u$ is given $\varphi$. Actually, to make that $\varphi = \langle \cdot, u\rangle$, for each $v=\sum_{i} \langle v, e_i\rangle e_i\in V$ ($e_i$'s forms a orthonormal basis of $V$), we must have $\varphi(v) = \langle v, u\rangle$. Then:
    $$
    \begin{aligned}
        \varphi(v) &= \sum \langle v, e_i\rangle \varphi(e_i)\\
        &= \sum \langle v, \overline{\varphi(e_i)} e_i\rangle\\
        &= \langle v, \sum \overline{\varphi(e_i)} e_i\rangle.
    \end{aligned}
    $$
    \textcolor{Th}{Hence, the vector $u$ in the Riesz representation theorem is $\sum \overline{\varphi(e_i)} e_i$.}
    Here we can also see that \textcolor{Th}{the vector $\sum \overline{\varphi(e_i)} e_i$ is invariant under the choice of the orthonormal basis.}
\end{Rmk}

\begin{Df}{$\bullet$ Df7.6 (orthogonal subspaces)}
    Suppose $V$ is an inner product space and $U, W$ are both subspaces of $V$. Then $U$ and $W$ are said to be orthogonal, denoted as $U\perp W$, if $\forall u\in U, \forall w\in W, u\perp w$.
\end{Df}

\begin{Rmk}{}
    \textcolor{Th}{In a finite-dimensional inner product space, two subspaces spanned by disjoint sublists of some orthogonal list of vectors are orthogonal. And the orthogonal subspaces in a finite-dimensional inner product space always appear in this way.} Since orthogonality implies linear independence, we can see that \textcolor{Th}{if $U\perp W$, then $U+W$ is a direct sum.} And the result similar to the theorem \{, ID: 7.3.1.1\} holds: \textcolor{Th}{if $U \perp W_1, \dots, W_m$ respectively, then $U\perp (W_1\oplus\cdots\oplus W_m)$.} \textcolor{Df}{When a one-dimensional subspace $U = \text{span}(u)$ is orthogonal to a subspace $W$, we also say that the vector $u$ is orthogonal to $W$.} We now define the orthogonal complement of a subspace.
\end{Rmk}

\begin{Df}{$\bullet$ Df7.6.1 (orthogonal complement)}
    Suppose $V$ is an inner product space and $U$ is a subspace of $V$. The orthogonal complement of $U$, denoted as $U^\perp$, is defined as the subspace $U^\perp$ of $V$ such that $U\perp U^\perp$ and $U\oplus U^\perp = V$.
\end{Df}

\begin{Rmk}{}
    To well-define the orthogonal complement, we need to ensure that such $U^\perp$ exists and is unique. \textcolor{Th}{Yes, exactly. Actually we can extend some orthogonal basis of $U$ to an orthogonal basis of $V$ and take the supplemented part as $U^\perp$ to show the existence, and verify that such $U^\perp$ must be $\{u^\perp\in V: u^\perp\perp U\}$ to show the uniqueness.} From the discussion just now, we can see that \textcolor{Th}{in a finite-dimensional inner product space, take the orthogonal complement of a subspace is just to choose all the other perpendicular dimensions.} With this insight, we can easily verify the following basic properties:
    \textcolor{Th}{Suppose $V$ is a finite-dimensional inner product space and $U, W$ are subspaces of $V$. Then:
        \begin{compactenum}
            \item $(U^\perp)^{\perp} = U$;
            \item $\{0\}^\perp = V$;
            \item If $U\subseteq W$, then $U^\perp\supseteq W^\perp$;
        \end{compactenum}
    }
\end{Rmk}

\begin{Th}{$\bullet$ Th7.6.1.1 (the row space is orthogonal to the null space)}
    Suppose $A$ is a matrix over $\mathbb{R}$. Then under the dot product inner product:
    \begin{compactenum}
        \item $\text{Row}(A) = \text{Null}(A)^\perp$;
        \item $\text{Col}(A) = \text{LNull}(A)^\perp$.
    \end{compactenum}
    \tcblower
    \textit{Pf}: Trivial.
\end{Th}

\begin{Df}{$\bullet$ Df7.6.2 (orthogonal projection)}
    Suppose $V$ is a finite-dimensional inner product space and $U$ is a subspace of $V$. Then for each vector $v\in V$, the orthogonal projection of $v$ onto $U$, denoted as $\text{Proj}(v|U)$, is the vector $\text{Proj}(v|U) = u$ where $v = u+u^\perp$, $u\in U$ and $u^\perp\in U^\perp$. \textcolor{Th}{This operator $\text{Proj}(\cdot|U)$ is verified to be linear}, and we call it the orthogonal projection operator onto $U$.
\end{Df}

\begin{Rmk}{}
    \textcolor{Th}{Recall that the theorem \{, ID: 7.3.3\} is a special case of orthogonal projection here where $U = \text{span}(u)$ is one-dimensional.} And we can verify the following basic properties of orthogonal projection operator $\text{Proj}(\cdot |U)$: 
    \textcolor{Th}{
    \begin{compactenum}
        \item $\text{Proj}(u|U) = u$ for all $u\in U$, $\text{Proj}(u^\perp|U) = 0$ for all $u^\perp\in U^\perp$;
        \item $R(\text{Proj}(\cdot |U)) = U$, $N(\text{Proj}(\cdot |U)) = U^\perp$;
        \item $v-\text{Proj}(v|U)\in U^\perp$ for all $v\in V$;
        \item $\text{Proj}^2(\cdot |U) = \text{Proj}(\cdot |U)$;
        \item $\|\text{Proj}(v|U)\| \leq \|v\|$ for all $v\in V$.
    \end{compactenum}
    }
\end{Rmk}

\begin{Th}{$\bullet$ Th7.6.2.1 (conditions of orthogonal projection)}
    Suppose $V$ is a finite-dimensional inner product space and $P\in\mathcal{L}(V)$. Then $P$ is an orthogonal projection operator iff there exists an orthogonal decomposition $V = U\oplus W$ (where $W = U^\perp$) s.t. $P|_U = I$ and $P|_W = 0$.
    \tcblower
    \textit{Pf}: Trivial.
\end{Th}

\begin{Th}{$\bullet$ Th7.6.3 (orthogonal projection and minimization)}
    Suppose $V$ is a finite-dimensional vector space and $U$ is a subspace of $V$. Then for each $v\in V$, we have:
    $$\forall u\in U, \; \|v-\text{Proj}(v|U)\|\leq \|v-u\|,$$
    and the equality holds iff $u=\text{Proj}(v|U)$. 
    \tcblower
    \textit{Pf}: Square two sides and decompose the squared norm into every orthonormal dimension (using multi-dimensional Pythagorean theorem in the remark \{, ID: 7.4\}).
\end{Th}

\begin{Rmk}{}
    This theorem is natural (very natural in geometrical intuition) but commonly applied. The most typical application is in the least square estimate in the statistical linear model.
\end{Rmk}

\begin{Df}{$\bullet$ Df7.7 (projection operator)}
    In a finite-dimensional inner product space, an idempotent operator $P$ (i.e. $P^2 = P$) is called a projection operator.
\end{Df}

\begin{Rmk}{}
    This definition does not appear out of nowhere. We now give a derivation of this definition. 

    The concept of projection operator is for depicting the projection phenomenon in our lives. Imagine that one end of a rod is fixed on the ground and the rod is at some angle with the ground. If the parallel light shines above the rod in a certain direction, the rod will be projected onto the ground and form a shadow. If the rod is already lying on the ground, then it will definitely coinside with its shadow. This observation of projection phenomenon naturally inspires us the necessary and sufficient conditions of a projection operator $P$:
    \begin{compactenum}
        \item \textcolor{Th}{$P$ is linear,} as the light does not bend so that the shadow is additive and scaler-multiplicative;
        \item \textcolor{Th}{There exists a subspace $U$ (called the projection space) s.t. $P|_U = I$ and $R(P) = U$,} as the projection plane is invariant under projection.
    \end{compactenum}
    And we see that there are two essentials of a projection operator: 1. A projection space (the ground), 2. A direct-sum-complement of the projection space (the direction of the light).

    Let us transform the conditions of projection operator above equivalently:
    \begin{compactenum}
        \item \textcolor{Th}{$P$ is linear;}
        \item \textcolor{Th}{$P|_{R(P)} = I$.}
    \end{compactenum}
    or:
    \begin{compactenum}
        \item \textcolor{Th}{$P$ is linear;}
        \item \textcolor{Th}{$P$ is idempotent,} as $P^2v \overset{\forall v}{=} Pv \;\Leftrightarrow\; P(Pv) \overset{\forall v}{=} Pv \;\Leftrightarrow\; P|_{R(P)} = I$.
    \end{compactenum}
    Hence this idempotence definition for projection operator is indeed an equivalent definition, or say ``a rational modeling'' for the projection phenomenon.

    So far you must have noticed that \textcolor{Th}{the orthogonal projection operator is a special case of projection operator defined here.} And from the geometrical intuition, orthogonal projection occurs when the light shines down from directly above. Then we can get another perspective of the projection operator â€” the projection is just to keep the dimensions on the projection plane and remove the dimensions of the light:
    \textcolor{Th}{An operator $P$ is a projection operator iff:
    \begin{compactenum}
        \item $P$ is linear;
        \item There is a direct-sum decomposition $V = U\oplus W$ s.t. $P|_U = I$ and $P|_W = 0$
    \end{compactenum}    
    }
    This gives us a clear comparison with the theorem \{, ID: 7.6.2.1\}. Hence \textcolor{Df}{the projection can be classified into orthogonal projection and oblique projection, according to whether $U\perp W$.}
\end{Rmk}

\begin{Th}{$\bullet$ Th7.7.1 (constructure of projection operator)}
    Suppose $V$ is a finite-dimensional inner product space and $P\in\mathcal{L}(V)$. Then:\\
    $P$ is a projection operator (resp. orthogonal projection operator) iff there is some basis (resp. orthogonal basis) $\{u_1, \dots, u_m, w_1, \dots, w_n\}$ of $V$ ($m, n\in\mathbb{N}$) s.t. $P$ is the operator that only keeps the $u$'s components of each $v\in V$. 
    \tcblower
    \textit{Pf}: Trivial.
\end{Th}

\begin{Rmk}{}
    \textcolor{Df}{We can denote a projection operator as $\text{Proj}(\cdot|U\oplus W)$, where $U$ is the projection space and $W$ is the direct-sum-complement this projection operator is based on. If we omit $W$, then this notation refers to the orthogonal projection operator by default.} 
\end{Rmk}

\end{document}